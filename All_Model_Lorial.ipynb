{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a33103c97b4d41e7b2425cc95a147ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_516a0b5beed144f0bf0879dd23e2f4f2",
              "IPY_MODEL_e4ce911b01f647e2b95b7c04a2e99df6",
              "IPY_MODEL_995ece097c6745539d40f2dadfcea0c0"
            ],
            "layout": "IPY_MODEL_4bd0e132bf894867882f2e5e10c79351"
          }
        },
        "516a0b5beed144f0bf0879dd23e2f4f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e9953d6aa39468ca518bd179cbaf6a3",
            "placeholder": "​",
            "style": "IPY_MODEL_a8849e2d8d6a4e16b91bfe4f7f3d4f24",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "e4ce911b01f647e2b95b7c04a2e99df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c09858832b7f4c6099e89a1b72745af8",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05b2dd364e34475899a6b90d0668a260",
            "value": 25
          }
        },
        "995ece097c6745539d40f2dadfcea0c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e19deda79dd4b08958b8f906b66ea7c",
            "placeholder": "​",
            "style": "IPY_MODEL_68a80e4518e245bebfa91c96f11a57a8",
            "value": " 25.0/25.0 [00:00&lt;00:00, 1.09kB/s]"
          }
        },
        "4bd0e132bf894867882f2e5e10c79351": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e9953d6aa39468ca518bd179cbaf6a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8849e2d8d6a4e16b91bfe4f7f3d4f24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c09858832b7f4c6099e89a1b72745af8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05b2dd364e34475899a6b90d0668a260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e19deda79dd4b08958b8f906b66ea7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68a80e4518e245bebfa91c96f11a57a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e151a284d02642b7b32dd25a347044e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bee504562e68414394de2f70848c4b63",
              "IPY_MODEL_95475555304d45cb91cf9dd3bb681c73",
              "IPY_MODEL_1420a802ca00484aa33dc95c232c1db5"
            ],
            "layout": "IPY_MODEL_f6d18f69ce97455fb820d20fa6067d2b"
          }
        },
        "bee504562e68414394de2f70848c4b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_678c1aef7aab48be9128c64259c313bb",
            "placeholder": "​",
            "style": "IPY_MODEL_cc47dd3b96d94519988ecd4d8f753bb4",
            "value": "vocab.json: 100%"
          }
        },
        "95475555304d45cb91cf9dd3bb681c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c11dcd593ce847ab8a04930fe47d0c5a",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a6c53ce375b40618ba6cd7fe0cdc219",
            "value": 898823
          }
        },
        "1420a802ca00484aa33dc95c232c1db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f8baaa8abbe4c129a2a16ea94dfe88e",
            "placeholder": "​",
            "style": "IPY_MODEL_551e5afc227244cc84e2a7bdd674011b",
            "value": " 899k/899k [00:00&lt;00:00, 4.57MB/s]"
          }
        },
        "f6d18f69ce97455fb820d20fa6067d2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "678c1aef7aab48be9128c64259c313bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc47dd3b96d94519988ecd4d8f753bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c11dcd593ce847ab8a04930fe47d0c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a6c53ce375b40618ba6cd7fe0cdc219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f8baaa8abbe4c129a2a16ea94dfe88e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "551e5afc227244cc84e2a7bdd674011b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27e1ac28ce524be8b558e9dbb2096a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b95fff5a40647f382a70e7e795215a0",
              "IPY_MODEL_cf38fdface1e4e9b8228d5c7529719b5",
              "IPY_MODEL_3f505422d2db4778891ed12d4eb66263"
            ],
            "layout": "IPY_MODEL_d1aa9e5fb95a4f43b6ccac312899cedb"
          }
        },
        "2b95fff5a40647f382a70e7e795215a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbaeb0be0c3f4322acff1941fb1f8f05",
            "placeholder": "​",
            "style": "IPY_MODEL_dc9b8b942aff44188785f89bd04dcf02",
            "value": "merges.txt: 100%"
          }
        },
        "cf38fdface1e4e9b8228d5c7529719b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fea80a37f6254a64a608c52e4750b50e",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef2c02bdd68547ca86db8ebbaa751a73",
            "value": 456318
          }
        },
        "3f505422d2db4778891ed12d4eb66263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_025c7bd5d75b4904819d78469d58a8ed",
            "placeholder": "​",
            "style": "IPY_MODEL_84e32786ca8844e09b3f285fb9f76cea",
            "value": " 456k/456k [00:00&lt;00:00, 5.85MB/s]"
          }
        },
        "d1aa9e5fb95a4f43b6ccac312899cedb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbaeb0be0c3f4322acff1941fb1f8f05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc9b8b942aff44188785f89bd04dcf02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fea80a37f6254a64a608c52e4750b50e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef2c02bdd68547ca86db8ebbaa751a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "025c7bd5d75b4904819d78469d58a8ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84e32786ca8844e09b3f285fb9f76cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f502c7e99b814205976c4fbea79f821e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_301ddfcacd9448b0b29867c2ec996b90",
              "IPY_MODEL_6b1624c356bd4b60920a9f850d53e79d",
              "IPY_MODEL_6a4f23864dec4bcca2c285813d2678b2"
            ],
            "layout": "IPY_MODEL_67f8e576424d4c108231f04a0d3296c2"
          }
        },
        "301ddfcacd9448b0b29867c2ec996b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c72b616b2fbe4a8e8e54c0c7951c78d0",
            "placeholder": "​",
            "style": "IPY_MODEL_8adf72cabd8349ea9e5e3d23e4cadd73",
            "value": "tokenizer.json: 100%"
          }
        },
        "6b1624c356bd4b60920a9f850d53e79d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a4dbb026e824809813ecfefa2a70dfe",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fb1edb9ae194d628db60b964a20dfec",
            "value": 1355863
          }
        },
        "6a4f23864dec4bcca2c285813d2678b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1beafafdcf9c4a78aa5e1302f8b7438a",
            "placeholder": "​",
            "style": "IPY_MODEL_239c2692a53d4bd0b83a966c9bb2c532",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 18.2MB/s]"
          }
        },
        "67f8e576424d4c108231f04a0d3296c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c72b616b2fbe4a8e8e54c0c7951c78d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8adf72cabd8349ea9e5e3d23e4cadd73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a4dbb026e824809813ecfefa2a70dfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fb1edb9ae194d628db60b964a20dfec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1beafafdcf9c4a78aa5e1302f8b7438a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "239c2692a53d4bd0b83a966c9bb2c532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a84ad5372ff485dbde55a7dd7871023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_adada39a40a44e379bd17caf9af7a557",
              "IPY_MODEL_4e78df3fd7a34c11920d561a637007bc",
              "IPY_MODEL_c18956819c514e1ba65ff612b3c03611"
            ],
            "layout": "IPY_MODEL_d9013517dcf448c596ab2f57bd58cf03"
          }
        },
        "adada39a40a44e379bd17caf9af7a557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f2559d2d53d4c0380279dce7b453d3f",
            "placeholder": "​",
            "style": "IPY_MODEL_222303cf14ef409c999b4b63b013dafe",
            "value": "config.json: 100%"
          }
        },
        "4e78df3fd7a34c11920d561a637007bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85073ae3e0e248a4bb850175a68e6d76",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_954140d74de94b7e8044c1362e2b6782",
            "value": 481
          }
        },
        "c18956819c514e1ba65ff612b3c03611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9490e4b110724803905128317f686655",
            "placeholder": "​",
            "style": "IPY_MODEL_3a458108b59a4ae798309626a8adce94",
            "value": " 481/481 [00:00&lt;00:00, 9.21kB/s]"
          }
        },
        "d9013517dcf448c596ab2f57bd58cf03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f2559d2d53d4c0380279dce7b453d3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "222303cf14ef409c999b4b63b013dafe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85073ae3e0e248a4bb850175a68e6d76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "954140d74de94b7e8044c1362e2b6782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9490e4b110724803905128317f686655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a458108b59a4ae798309626a8adce94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e182ad6dcedb4eff961dcd8ce4da3c0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a8559f1cb974683a5a85673829e938b",
              "IPY_MODEL_0866c4abf29044c8954c037893716c4e",
              "IPY_MODEL_7f94e1c87fc043068c3d9f21a181dc8e"
            ],
            "layout": "IPY_MODEL_3f79b87071cb41c0939e03d04fc149d4"
          }
        },
        "3a8559f1cb974683a5a85673829e938b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87f5356fff434f218fe6f9269ea6c6e6",
            "placeholder": "​",
            "style": "IPY_MODEL_3abf02d9231b40838b047670d9bbd451",
            "value": "model.safetensors: 100%"
          }
        },
        "0866c4abf29044c8954c037893716c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f48a393179334ee8ad3aa31c84e42311",
            "max": 498818054,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56be98d5e72d4ab6a873f57d40b4366b",
            "value": 498818054
          }
        },
        "7f94e1c87fc043068c3d9f21a181dc8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43e8aa40c579483baf31c55ed56111e6",
            "placeholder": "​",
            "style": "IPY_MODEL_f386137e402c4f049c44c10ff73619dc",
            "value": " 499M/499M [00:05&lt;00:00, 41.0MB/s]"
          }
        },
        "3f79b87071cb41c0939e03d04fc149d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87f5356fff434f218fe6f9269ea6c6e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3abf02d9231b40838b047670d9bbd451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f48a393179334ee8ad3aa31c84e42311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56be98d5e72d4ab6a873f57d40b4366b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43e8aa40c579483baf31c55ed56111e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f386137e402c4f049c44c10ff73619dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn transformers lightgbm codecarbon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjg2fLKSslUJ",
        "outputId": "4d23a725-a324-48e2-9814-69bcd07b5685"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Collecting codecarbon\n",
            "  Downloading codecarbon-2.8.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Collecting arrow (from codecarbon)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from codecarbon) (8.1.8)\n",
            "Collecting fief-client[cli] (from codecarbon)\n",
            "  Downloading fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.21.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from codecarbon) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from codecarbon) (9.0.0)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from codecarbon) (12.0.0)\n",
            "Collecting questionary (from codecarbon)\n",
            "  Downloading questionary-2.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting rapidfuzz (from codecarbon)\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from codecarbon) (13.9.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.15.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->codecarbon)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting httpx<0.28.0,>=0.21.3 (from fief-client[cli]->codecarbon)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon)\n",
            "  Downloading jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting yaspin (from fief-client[cli]->codecarbon)\n",
            "  Downloading yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->codecarbon) (12.570.86)\n",
            "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from questionary->codecarbon) (3.0.50)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon) (2.18.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->codecarbon) (1.5.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
            "Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.11/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.13)\n",
            "Collecting termcolor<2.4.0,>=2.2.0 (from yaspin->fief-client[cli]->codecarbon)\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n",
            "Downloading codecarbon-2.8.3-py3-none-any.whl (516 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.7/516.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading questionary-2.1.0-py3-none-any.whl (36 kB)\n",
            "Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Downloading fief_client-0.20.0-py3-none-any.whl (20 kB)\n",
            "Downloading yaspin-3.1.0-py3-none-any.whl (18 kB)\n",
            "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: types-python-dateutil, termcolor, rapidfuzz, yaspin, questionary, httpx, arrow, jwcrypto, fief-client, codecarbon\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.5.0\n",
            "    Uninstalling termcolor-2.5.0:\n",
            "      Successfully uninstalled termcolor-2.5.0\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.4.0 requires httpx<1.0.0dev,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arrow-1.3.0 codecarbon-2.8.3 fief-client-0.20.0 httpx-0.27.2 jwcrypto-1.5.6 questionary-2.1.0 rapidfuzz-3.12.2 termcolor-2.3.0 types-python-dateutil-2.9.0.20241206 yaspin-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regression + LightGBM 2 - Best Model **"
      ],
      "metadata": {
        "id": "9MjDBRqZJg8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import lightgbm as lgb\n",
        "from codecarbon import EmissionsTracker\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Suppress scikit-learn deprecation warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Disable CodeCarbon logs\n",
        "logging.getLogger(\"codecarbon\").setLevel(logging.WARNING)\n",
        "\n",
        "# Delete the lock file if it exists\n",
        "lock_file = \"/tmp/.codecarbon.lock\"\n",
        "if os.path.exists(lock_file):\n",
        "    os.remove(lock_file)\n",
        "\n",
        "# Initialize CodeCarbon tracker (allow multiple runs)\n",
        "tracker = EmissionsTracker(log_level=\"error\", allow_multiple_runs=True)  # Suppress all logs except errors\n",
        "tracker.start()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/anthropic.claude-3-5-sonnet Full.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Enhanced Text Cleaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # Remove stopwords and lemmatize\n",
        "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df[\"text_raw\"] = df[\"text_raw\"].apply(clean_text)\n",
        "\n",
        "# Features: 'text_raw' column\n",
        "X = df[\"text_raw\"]\n",
        "\n",
        "# Labels: Binary attributes (columns 1 to 33)\n",
        "binary_columns = df.columns[1:34]  # Assuming columns 1 to 33 are binary attributes\n",
        "y = df[binary_columns]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Vectorize text data using TF-IDF with character n-grams\n",
        "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))  # Reduced max_features\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression + LightGBM ensemble for each binary attribute\n",
        "results_ensemble = []\n",
        "y_pred_all = []  # To store predictions for all attributes\n",
        "\n",
        "for i, col in enumerate(binary_columns):\n",
        "    print(f\"Training Ensemble (Logistic Regression + LightGBM) for attribute: {col}\")\n",
        "\n",
        "    # Calculate class weights for imbalanced data\n",
        "    class_weights = {0: 1, 1: len(y_train[col]) / sum(y_train[col])}  # Higher weight for minority class\n",
        "\n",
        "    # Define Logistic Regression model\n",
        "    lr_model = LogisticRegression(\n",
        "        class_weight=\"balanced\",  # Handle imbalanced classes\n",
        "        max_iter=500,           # Reduced iterations\n",
        "        n_jobs=-1,               # Use all cores\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Define LightGBM model\n",
        "    lgb_model = lgb.LGBMClassifier(\n",
        "        n_estimators=100,  # Reduced number of trees\n",
        "        learning_rate=0.1,  # Slightly higher learning rate for faster convergence\n",
        "        max_depth=3,        # Reduced depth to prevent overfitting\n",
        "        random_state=42,\n",
        "        n_jobs=-1,          # Use all available CPU cores\n",
        "        class_weight=class_weights,  # Handle class imbalance\n",
        "        verbosity=-1,  # Suppress LightGBM warnings\n",
        "        subsample=0.8,  # Subsample to reduce computation\n",
        "        colsample_bytree=0.8  # Feature subsampling to reduce computation\n",
        "    )\n",
        "\n",
        "    # Create Voting Classifier (ensemble of Logistic Regression and LightGBM)\n",
        "    ensemble_model = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('lr', lr_model),\n",
        "            ('lgb', lgb_model)\n",
        "        ],\n",
        "        voting='soft',  # Use soft voting for probabilistic predictions\n",
        "        n_jobs=-1       # Use all cores\n",
        "    )\n",
        "\n",
        "    # Train the ensemble model\n",
        "    ensemble_model.fit(X_train_tfidf, y_train[col])\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = ensemble_model.predict(X_test_tfidf)\n",
        "    y_pred_all.append(y_pred)  # Store predictions\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test[col], y_pred)\n",
        "    f1 = f1_score(y_test[col], y_pred, average='binary')\n",
        "    recall = recall_score(y_test[col], y_pred, average='binary')\n",
        "    precision = precision_score(y_test[col], y_pred, average='binary')\n",
        "\n",
        "    results_ensemble.append([col, accuracy, f1, recall, precision])\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_ensemble_df = pd.DataFrame(results_ensemble, columns=[\"Attribute\", \"Accuracy\", \"F1\", \"Recall\", \"Precision\"])\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_accuracy_ensemble = results_ensemble_df[\"Accuracy\"].mean()\n",
        "avg_f1_ensemble = results_ensemble_df[\"F1\"].mean()\n",
        "avg_recall_ensemble = results_ensemble_df[\"Recall\"].mean()\n",
        "avg_precision_ensemble = results_ensemble_df[\"Precision\"].mean()\n",
        "\n",
        "# Add average row to the results table\n",
        "results_ensemble_df.loc[\"Average\"] = [\"Average\", avg_accuracy_ensemble, avg_f1_ensemble, avg_recall_ensemble, avg_precision_ensemble]\n",
        "\n",
        "# Display results\n",
        "print(\"Ensemble (Logistic Regression + LightGBM) Results:\")\n",
        "print(results_ensemble_df)\n",
        "\n",
        "# Convert predictions to a 2D array (num_samples, num_attributes)\n",
        "y_pred_all = np.array(y_pred_all).T\n",
        "\n",
        "# Generate classification report for micro, macro, weighted, and samples averages\n",
        "classification_report_result = classification_report(\n",
        "    y_test, y_pred_all, target_names=binary_columns, output_dict=True\n",
        ")\n",
        "\n",
        "# Extract micro, macro, weighted, and samples averages\n",
        "micro_avg = classification_report_result['micro avg']\n",
        "macro_avg = classification_report_result['macro avg']\n",
        "weighted_avg = classification_report_result['weighted avg']\n",
        "samples_avg = classification_report_result['samples avg']\n",
        "\n",
        "# Display the averages\n",
        "print(\"\\nMicro Average:\")\n",
        "print(f\"Precision: {micro_avg['precision']:.2f}, Recall: {micro_avg['recall']:.2f}, F1: {micro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nMacro Average:\")\n",
        "print(f\"Precision: {macro_avg['precision']:.2f}, Recall: {macro_avg['recall']:.2f}, F1: {macro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nWeighted Average:\")\n",
        "print(f\"Precision: {weighted_avg['precision']:.2f}, Recall: {weighted_avg['recall']:.2f}, F1: {weighted_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nSamples Average:\")\n",
        "print(f\"Precision: {samples_avg['precision']:.2f}, Recall: {samples_avg['recall']:.2f}, F1: {samples_avg['f1-score']:.2f}\")\n",
        "\n",
        "# Stop the CodeCarbon tracker and get the emissions\n",
        "emissions = tracker.stop()\n",
        "if emissions is None:\n",
        "    emissions = 0.0  # Default value if tracker fails\n",
        "\n",
        "print(f\"\\nTotal Carbon Emissions: {emissions:.4f} kg CO2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w_eGKEXOgft",
        "outputId": "ddf3344b-b8ce-4fc2-def0-539d2bb05359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[codecarbon WARNING @ 17:22:14] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: dark_pigmentation\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: acne\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: eye_contour\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: homogeneity\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: lack_firmness\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: lack_radiance\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: pores\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: fine_lines\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: wrinkles_fine-lines\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: eye-wrinkles\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: undereye-bags\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: generic\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: 18-34\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: 35-54\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: 55-99\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: dry\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: normal\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: oily\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: combination\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: sensitivity-high\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: sensitivity-low\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: no_sensitivity\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: male\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: female\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: cleanse\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: prepare\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: treat\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: targeted\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: care\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: moisturize\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: protect\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: day\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: night\n",
            "Ensemble (Logistic Regression + LightGBM) Results:\n",
            "                   Attribute  Accuracy        F1    Recall  Precision\n",
            "0          dark_pigmentation  0.951923  0.824903  0.887029   0.770909\n",
            "1                       acne  0.980769  0.910448  0.901478   0.919598\n",
            "2                eye_contour  0.979167  0.913525  0.962617   0.869198\n",
            "3                homogeneity  0.916667  0.745098  0.880309   0.645892\n",
            "4              lack_firmness  0.917201  0.776978  0.857143   0.710526\n",
            "5              lack_radiance  0.899573  0.854714  0.858696   0.850769\n",
            "6                      pores  0.933226  0.858437  0.893868   0.825708\n",
            "7                 fine_lines  0.911859  0.860994  0.875000   0.847430\n",
            "8        wrinkles_fine-lines  0.919872  0.888724  0.887407   0.890045\n",
            "9               eye-wrinkles  0.972222  0.865979  0.933333   0.807692\n",
            "10             undereye-bags  0.978632  0.887640  0.946108   0.835979\n",
            "11                   generic  0.778846  0.832253  0.912889   0.764706\n",
            "12                     18-34  0.828526  0.865859  0.920889   0.817035\n",
            "13                     35-54  0.827991  0.811254  0.832732   0.790857\n",
            "14                     55-99  0.867521  0.755424  0.827214   0.695100\n",
            "15                       dry  0.878205  0.818760  0.886403   0.760709\n",
            "16                    normal  0.798611  0.749168  0.829161   0.683252\n",
            "17                      oily  0.906517  0.804469  0.835267   0.775862\n",
            "18               combination  0.772436  0.710991  0.806154   0.635922\n",
            "19          sensitivity-high  0.912393  0.740506  0.823944   0.672414\n",
            "20           sensitivity-low  0.770299  0.369501  0.617647   0.263598\n",
            "21            no_sensitivity  0.782051  0.854494  0.938136   0.784545\n",
            "22                      male  0.973291  0.893162  0.881857   0.904762\n",
            "23                    female  0.826389  0.877774  0.911007   0.846880\n",
            "24                   cleanse  0.954060  0.890026  0.928000   0.855037\n",
            "25                   prepare  0.902778  0.766067  0.861272   0.689815\n",
            "26                     treat  0.874466  0.869372  0.891676   0.848156\n",
            "27                  targeted  0.887286  0.845194  0.879389   0.813559\n",
            "28                      care  0.746261  0.758270  0.838964   0.691736\n",
            "29                moisturize  0.892628  0.917454  0.927741   0.907392\n",
            "30                   protect  0.883013  0.817043  0.806931   0.827411\n",
            "31                       day  0.830128  0.894980  0.931271   0.861411\n",
            "32                     night  0.833333  0.863517  0.920709   0.813015\n",
            "Average              Average  0.881459  0.820999  0.875522   0.778089\n",
            "\n",
            "Micro Average:\n",
            "Precision: 0.79, Recall: 0.89, F1: 0.83\n",
            "\n",
            "Macro Average:\n",
            "Precision: 0.78, Recall: 0.88, F1: 0.82\n",
            "\n",
            "Weighted Average:\n",
            "Precision: 0.80, Recall: 0.89, F1: 0.84\n",
            "\n",
            "Samples Average:\n",
            "Precision: 0.78, Recall: 0.89, F1: 0.82\n",
            "\n",
            "Total Carbon Emissions: 0.0002 kg CO2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LightGBM**"
      ],
      "metadata": {
        "id": "eTD4La43e2aG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4DpXjvtLgOt",
        "outputId": "094ed6d6-4243-4db0-cd9c-d5b8cbc136d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Collecting codecarbon\n",
            "  Downloading codecarbon-2.8.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Collecting arrow (from codecarbon)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from codecarbon) (8.1.8)\n",
            "Collecting fief-client[cli] (from codecarbon)\n",
            "  Downloading fief_client-0.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.21.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from codecarbon) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from codecarbon) (9.0.0)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from codecarbon) (12.0.0)\n",
            "Collecting questionary (from codecarbon)\n",
            "  Downloading questionary-2.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting rapidfuzz (from codecarbon)\n",
            "  Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from codecarbon) (13.9.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.15.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->codecarbon)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting httpx<0.28.0,>=0.21.3 (from fief-client[cli]->codecarbon)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jwcrypto<2.0.0,>=1.4 (from fief-client[cli]->codecarbon)\n",
            "  Downloading jwcrypto-1.5.6-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting yaspin (from fief-client[cli]->codecarbon)\n",
            "  Downloading yaspin-3.1.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->codecarbon) (12.570.86)\n",
            "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from questionary->codecarbon) (3.0.50)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon) (2.18.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->codecarbon) (1.5.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
            "Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.11/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.13)\n",
            "Collecting termcolor<2.4.0,>=2.2.0 (from yaspin->fief-client[cli]->codecarbon)\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n",
            "Downloading codecarbon-2.8.3-py3-none-any.whl (516 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.7/516.7 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading questionary-2.1.0-py3-none-any.whl (36 kB)\n",
            "Downloading rapidfuzz-3.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jwcrypto-1.5.6-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Downloading fief_client-0.20.0-py3-none-any.whl (20 kB)\n",
            "Downloading yaspin-3.1.0-py3-none-any.whl (18 kB)\n",
            "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: types-python-dateutil, termcolor, rapidfuzz, yaspin, questionary, httpx, arrow, jwcrypto, fief-client, codecarbon\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.5.0\n",
            "    Uninstalling termcolor-2.5.0:\n",
            "      Successfully uninstalled termcolor-2.5.0\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.4.0 requires httpx<1.0.0dev,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed arrow-1.3.0 codecarbon-2.8.3 fief-client-0.20.0 httpx-0.27.2 jwcrypto-1.5.6 questionary-2.1.0 rapidfuzz-3.12.2 termcolor-2.3.0 types-python-dateutil-2.9.0.20241206 yaspin-3.1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 17:10:07] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LightGBM for attribute: dark_pigmentation\n",
            "Training LightGBM for attribute: acne\n",
            "Training LightGBM for attribute: eye_contour\n",
            "Training LightGBM for attribute: homogeneity\n",
            "Training LightGBM for attribute: lack_firmness\n",
            "Training LightGBM for attribute: lack_radiance\n",
            "Training LightGBM for attribute: pores\n",
            "Training LightGBM for attribute: fine_lines\n",
            "Training LightGBM for attribute: wrinkles_fine-lines\n",
            "Training LightGBM for attribute: eye-wrinkles\n",
            "Training LightGBM for attribute: undereye-bags\n",
            "Training LightGBM for attribute: generic\n",
            "Training LightGBM for attribute: 18-34\n",
            "Training LightGBM for attribute: 35-54\n",
            "Training LightGBM for attribute: 55-99\n",
            "Training LightGBM for attribute: dry\n",
            "Training LightGBM for attribute: normal\n",
            "Training LightGBM for attribute: oily\n",
            "Training LightGBM for attribute: combination\n",
            "Training LightGBM for attribute: sensitivity-high\n",
            "Training LightGBM for attribute: sensitivity-low\n",
            "Training LightGBM for attribute: no_sensitivity\n",
            "Training LightGBM for attribute: male\n",
            "Training LightGBM for attribute: female\n",
            "Training LightGBM for attribute: cleanse\n",
            "Training LightGBM for attribute: prepare\n",
            "Training LightGBM for attribute: treat\n",
            "Training LightGBM for attribute: targeted\n",
            "Training LightGBM for attribute: care\n",
            "Training LightGBM for attribute: moisturize\n",
            "Training LightGBM for attribute: protect\n",
            "Training LightGBM for attribute: day\n",
            "Training LightGBM for attribute: night\n",
            "LightGBM Results:\n",
            "                   Attribute  Accuracy        F1    Recall  Precision\n",
            "0          dark_pigmentation  0.965545  0.871642  0.901235   0.843931\n",
            "1                       acne  0.985577  0.930769  0.930769   0.930769\n",
            "2                eye_contour  0.982372  0.923077  0.910345   0.936170\n",
            "3                homogeneity  0.938301  0.781870  0.878981   0.704082\n",
            "4              lack_firmness  0.939904  0.838013  0.869955   0.808333\n",
            "5              lack_radiance  0.914263  0.875437  0.880562   0.870370\n",
            "6                      pores  0.927885  0.837545  0.846715   0.828571\n",
            "7                 fine_lines  0.908654  0.853846  0.864935   0.843038\n",
            "8        wrinkles_fine-lines  0.927885  0.898649  0.888641   0.908884\n",
            "9               eye-wrinkles  0.979167  0.896000  0.903226   0.888889\n",
            "10             undereye-bags  0.983173  0.910638  0.955357   0.869919\n",
            "11                   generic  0.751603  0.819137  0.953804   0.717791\n",
            "12                     18-34  0.793269  0.840149  0.944290   0.756696\n",
            "13                     35-54  0.819712  0.804857  0.868914   0.749596\n",
            "14                     55-99  0.875801  0.770370  0.817610   0.728291\n",
            "15                       dry  0.869391  0.801944  0.854922   0.755149\n",
            "16                    normal  0.794872  0.744511  0.838202   0.669659\n",
            "17                      oily  0.918269  0.821053  0.844765   0.798635\n",
            "18               combination  0.778045  0.711759  0.812352   0.633333\n",
            "19          sensitivity-high  0.926282  0.780952  0.832487   0.735426\n",
            "20           sensitivity-low  0.793269  0.317460  0.454545   0.243902\n",
            "21            no_sensitivity  0.773237  0.850974  0.959620   0.764428\n",
            "22                      male  0.979968  0.913495  0.936170   0.891892\n",
            "23                    female  0.820513  0.879050  0.950935   0.817269\n",
            "24                   cleanse  0.951122  0.876768  0.915612   0.841085\n",
            "25                   prepare  0.909455  0.768916  0.806867   0.734375\n",
            "26                     treat  0.854968  0.851029  0.896014   0.810345\n",
            "27                  targeted  0.892628  0.851111  0.903302   0.804622\n",
            "28                      care  0.753205  0.775510  0.909402   0.675985\n",
            "29                moisturize  0.893429  0.919831  0.947826   0.893443\n",
            "30                   protect  0.883013  0.816583  0.818640   0.814536\n",
            "31                       day  0.823718  0.895238  0.980188   0.823839\n",
            "32                     night  0.794071  0.836201  0.941176   0.752294\n",
            "Average              Average  0.881896  0.826193  0.879344   0.783198\n",
            "\n",
            "Micro Average:\n",
            "Precision: 0.78, Recall: 0.90, F1: 0.83\n",
            "\n",
            "Macro Average:\n",
            "Precision: 0.78, Recall: 0.88, F1: 0.83\n",
            "\n",
            "Weighted Average:\n",
            "Precision: 0.78, Recall: 0.90, F1: 0.84\n",
            "\n",
            "Samples Average:\n",
            "Precision: 0.76, Recall: 0.91, F1: 0.82\n",
            "\n",
            "Total Carbon Emissions: 0.0005 kg CO2\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn transformers lightgbm codecarbon\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import lightgbm as lgb\n",
        "from codecarbon import EmissionsTracker\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress scikit-learn deprecation warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Disable CodeCarbon logs\n",
        "logging.getLogger(\"codecarbon\").setLevel(logging.WARNING)\n",
        "\n",
        "# Delete the lock file if it exists\n",
        "lock_file = \"/tmp/.codecarbon.lock\"\n",
        "if os.path.exists(lock_file):\n",
        "    os.remove(lock_file)\n",
        "\n",
        "# Initialize CodeCarbon tracker (allow multiple runs)\n",
        "tracker = EmissionsTracker(log_level=\"error\", allow_multiple_runs=True)  # Suppress all logs except errors\n",
        "tracker.start()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/anthropic.claude-3-5-sonnet Full.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Clean the 'text_raw' column\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df[\"text_raw\"] = df[\"text_raw\"].apply(clean_text)\n",
        "\n",
        "# Features: 'text_raw' column\n",
        "X = df[\"text_raw\"]\n",
        "\n",
        "# Labels: Binary attributes (columns 1 to 33)\n",
        "binary_columns = df.columns[1:34]  # Assuming columns 1 to 33 are binary attributes\n",
        "y = df[binary_columns]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data using TF-IDF with fewer features\n",
        "vectorizer = TfidfVectorizer(max_features=3000)  # Reduced to 3000 features for efficiency\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train LightGBM model for each binary attribute with optimized hyperparameters\n",
        "results_lgbm = []\n",
        "y_pred_all = []  # To store predictions for all attributes\n",
        "\n",
        "for i, col in enumerate(binary_columns):\n",
        "    print(f\"Training LightGBM for attribute: {col}\")\n",
        "\n",
        "    # Calculate class weights for imbalanced data\n",
        "    class_weights = {0: 1, 1: len(y_train[col]) / sum(y_train[col])}  # Higher weight for minority class\n",
        "\n",
        "    # Train LightGBM model with optimized hyperparameters\n",
        "    model = lgb.LGBMClassifier(\n",
        "        n_estimators=150,  # Reduced number of trees\n",
        "        learning_rate=0.1,  # Slightly higher learning rate for faster convergence\n",
        "        max_depth=5,        # Reduced depth to prevent overfitting\n",
        "        random_state=42,\n",
        "        n_jobs=-1,          # Use all available CPU cores\n",
        "        class_weight=class_weights,  # Handle class imbalance\n",
        "        verbosity=-1,  # Suppress LightGBM warnings\n",
        "        subsample=0.8,  # Subsample to reduce computation\n",
        "        colsample_bytree=0.8  # Feature subsampling to reduce computation\n",
        "    )\n",
        "\n",
        "    # Use early stopping with callbacks\n",
        "    callbacks = [\n",
        "        lgb.early_stopping(stopping_rounds=10, verbose=False),  # Early stopping\n",
        "        lgb.log_evaluation(period=0)  # Disable logging\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        X_train_tfidf,\n",
        "        y_train[col],\n",
        "        eval_set=[(X_test_tfidf, y_test[col])],\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    y_pred_all.append(y_pred)  # Store predictions\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test[col], y_pred)\n",
        "    f1 = f1_score(y_test[col], y_pred, average='binary')\n",
        "    recall = recall_score(y_test[col], y_pred, average='binary')\n",
        "    precision = precision_score(y_test[col], y_pred, average='binary')\n",
        "\n",
        "    results_lgbm.append([col, accuracy, f1, recall, precision])\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_lgbm_df = pd.DataFrame(results_lgbm, columns=[\"Attribute\", \"Accuracy\", \"F1\", \"Recall\", \"Precision\"])\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_accuracy_lgbm = results_lgbm_df[\"Accuracy\"].mean()\n",
        "avg_f1_lgbm = results_lgbm_df[\"F1\"].mean()\n",
        "avg_recall_lgbm = results_lgbm_df[\"Recall\"].mean()\n",
        "avg_precision_lgbm = results_lgbm_df[\"Precision\"].mean()\n",
        "\n",
        "# Add average row to the results table\n",
        "results_lgbm_df.loc[\"Average\"] = [\"Average\", avg_accuracy_lgbm, avg_f1_lgbm, avg_recall_lgbm, avg_precision_lgbm]\n",
        "\n",
        "# Display results\n",
        "print(\"LightGBM Results:\")\n",
        "print(results_lgbm_df)\n",
        "\n",
        "# Convert predictions to a 2D array (num_samples, num_attributes)\n",
        "y_pred_all = np.array(y_pred_all).T\n",
        "\n",
        "# Generate classification report for micro, macro, weighted, and samples averages\n",
        "classification_report_result = classification_report(\n",
        "    y_test, y_pred_all, target_names=binary_columns, output_dict=True\n",
        ")\n",
        "\n",
        "# Extract micro, macro, weighted, and samples averages\n",
        "micro_avg = classification_report_result['micro avg']\n",
        "macro_avg = classification_report_result['macro avg']\n",
        "weighted_avg = classification_report_result['weighted avg']\n",
        "samples_avg = classification_report_result['samples avg']\n",
        "\n",
        "# Display the averages\n",
        "print(\"\\nMicro Average:\")\n",
        "print(f\"Precision: {micro_avg['precision']:.2f}, Recall: {micro_avg['recall']:.2f}, F1: {micro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nMacro Average:\")\n",
        "print(f\"Precision: {macro_avg['precision']:.2f}, Recall: {macro_avg['recall']:.2f}, F1: {macro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nWeighted Average:\")\n",
        "print(f\"Precision: {weighted_avg['precision']:.2f}, Recall: {weighted_avg['recall']:.2f}, F1: {weighted_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nSamples Average:\")\n",
        "print(f\"Precision: {samples_avg['precision']:.2f}, Recall: {samples_avg['recall']:.2f}, F1: {samples_avg['f1-score']:.2f}\")\n",
        "\n",
        "# Stop the CodeCarbon tracker and get the emissions\n",
        "emissions = tracker.stop()\n",
        "if emissions is None:\n",
        "    emissions = 0.0  # Default value if tracker fails\n",
        "\n",
        "print(f\"\\nTotal Carbon Emissions: {emissions:.4f} kg CO2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regression + LightGBM 1**"
      ],
      "metadata": {
        "id": "oL0YT8xYfBx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import lightgbm as lgb\n",
        "from codecarbon import EmissionsTracker\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Suppress scikit-learn deprecation warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Disable CodeCarbon logs\n",
        "logging.getLogger(\"codecarbon\").setLevel(logging.WARNING)\n",
        "\n",
        "# Delete the lock file if it exists\n",
        "lock_file = \"/tmp/.codecarbon.lock\"\n",
        "if os.path.exists(lock_file):\n",
        "    os.remove(lock_file)\n",
        "\n",
        "# Initialize CodeCarbon tracker (allow multiple runs)\n",
        "tracker = EmissionsTracker(log_level=\"error\", allow_multiple_runs=True)  # Suppress all logs except errors\n",
        "tracker.start()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/anthropic.claude-3-5-sonnet Full.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Enhanced Text Cleaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # Remove stopwords and lemmatize\n",
        "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df[\"text_raw\"] = df[\"text_raw\"].apply(clean_text)\n",
        "\n",
        "# Features: 'text_raw' column\n",
        "X = df[\"text_raw\"]\n",
        "\n",
        "# Labels: Binary attributes (columns 1 to 33)\n",
        "binary_columns = df.columns[1:34]  # Assuming columns 1 to 33 are binary attributes\n",
        "y = df[binary_columns]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data using TF-IDF with character n-grams\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))  # Use character n-grams\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression + LightGBM ensemble for each binary attribute\n",
        "results_ensemble = []\n",
        "y_pred_all = []  # To store predictions for all attributes\n",
        "\n",
        "for i, col in enumerate(binary_columns):\n",
        "    print(f\"Training Ensemble (Logistic Regression + LightGBM) for attribute: {col}\")\n",
        "\n",
        "    # Calculate class weights for imbalanced data\n",
        "    class_weights = {0: 1, 1: len(y_train[col]) / sum(y_train[col])}  # Higher weight for minority class\n",
        "\n",
        "    # Define Logistic Regression model\n",
        "    lr_model = LogisticRegression(\n",
        "        class_weight=\"balanced\",  # Handle imbalanced classes\n",
        "        max_iter=1000,           # Increase iterations for convergence\n",
        "        n_jobs=-1,               # Use all cores\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Define LightGBM model\n",
        "    lgb_model = lgb.LGBMClassifier(\n",
        "        n_estimators=150,  # Reduced number of trees\n",
        "        learning_rate=0.1,  # Slightly higher learning rate for faster convergence\n",
        "        max_depth=5,        # Reduced depth to prevent overfitting\n",
        "        random_state=42,\n",
        "        n_jobs=-1,          # Use all available CPU cores\n",
        "        class_weight=class_weights,  # Handle class imbalance\n",
        "        verbosity=-1,  # Suppress LightGBM warnings\n",
        "        subsample=0.8,  # Subsample to reduce computation\n",
        "        colsample_bytree=0.8  # Feature subsampling to reduce computation\n",
        "    )\n",
        "\n",
        "    # Create Voting Classifier (ensemble of Logistic Regression and LightGBM)\n",
        "    ensemble_model = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('lr', lr_model),\n",
        "            ('lgb', lgb_model)\n",
        "        ],\n",
        "        voting='soft',  # Use soft voting for probabilistic predictions\n",
        "        n_jobs=-1       # Use all cores\n",
        "    )\n",
        "\n",
        "    # Train the ensemble model\n",
        "    ensemble_model.fit(X_train_tfidf, y_train[col])\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = ensemble_model.predict(X_test_tfidf)\n",
        "    y_pred_all.append(y_pred)  # Store predictions\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test[col], y_pred)\n",
        "    f1 = f1_score(y_test[col], y_pred, average='binary')\n",
        "    recall = recall_score(y_test[col], y_pred, average='binary')\n",
        "    precision = precision_score(y_test[col], y_pred, average='binary')\n",
        "\n",
        "    results_ensemble.append([col, accuracy, f1, recall, precision])\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_ensemble_df = pd.DataFrame(results_ensemble, columns=[\"Attribute\", \"Accuracy\", \"F1\", \"Recall\", \"Precision\"])\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_accuracy_ensemble = results_ensemble_df[\"Accuracy\"].mean()\n",
        "avg_f1_ensemble = results_ensemble_df[\"F1\"].mean()\n",
        "avg_recall_ensemble = results_ensemble_df[\"Recall\"].mean()\n",
        "avg_precision_ensemble = results_ensemble_df[\"Precision\"].mean()\n",
        "\n",
        "# Add average row to the results table\n",
        "results_ensemble_df.loc[\"Average\"] = [\"Average\", avg_accuracy_ensemble, avg_f1_ensemble, avg_recall_ensemble, avg_precision_ensemble]\n",
        "\n",
        "# Display results\n",
        "print(\"Ensemble (Logistic Regression + LightGBM) Results:\")\n",
        "print(results_ensemble_df)\n",
        "\n",
        "# Convert predictions to a 2D array (num_samples, num_attributes)\n",
        "y_pred_all = np.array(y_pred_all).T\n",
        "\n",
        "# Generate classification report for micro, macro, weighted, and samples averages\n",
        "classification_report_result = classification_report(\n",
        "    y_test, y_pred_all, target_names=binary_columns, output_dict=True\n",
        ")\n",
        "\n",
        "# Extract micro, macro, weighted, and samples averages\n",
        "micro_avg = classification_report_result['micro avg']\n",
        "macro_avg = classification_report_result['macro avg']\n",
        "weighted_avg = classification_report_result['weighted avg']\n",
        "samples_avg = classification_report_result['samples avg']\n",
        "\n",
        "# Display the averages\n",
        "print(\"\\nMicro Average:\")\n",
        "print(f\"Precision: {micro_avg['precision']:.2f}, Recall: {micro_avg['recall']:.2f}, F1: {micro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nMacro Average:\")\n",
        "print(f\"Precision: {macro_avg['precision']:.2f}, Recall: {macro_avg['recall']:.2f}, F1: {macro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nWeighted Average:\")\n",
        "print(f\"Precision: {weighted_avg['precision']:.2f}, Recall: {weighted_avg['recall']:.2f}, F1: {weighted_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nSamples Average:\")\n",
        "print(f\"Precision: {samples_avg['precision']:.2f}, Recall: {samples_avg['recall']:.2f}, F1: {samples_avg['f1-score']:.2f}\")\n",
        "\n",
        "# Stop the CodeCarbon tracker and get the emissions\n",
        "emissions = tracker.stop()\n",
        "if emissions is None:\n",
        "    emissions = 0.0  # Default value if tracker fails\n",
        "\n",
        "print(f\"\\nTotal Carbon Emissions: {emissions:.4f} kg CO2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qgJShi0Mvbx",
        "outputId": "d15d639d-1c76-4338-bbfb-27a33732ad6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[codecarbon WARNING @ 18:30:02] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: dark_pigmentation\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: acne\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: eye_contour\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: homogeneity\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: lack_firmness\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: lack_radiance\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: pores\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: fine_lines\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: wrinkles_fine-lines\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: eye-wrinkles\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: undereye-bags\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: generic\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: 18-34\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: 35-54\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: 55-99\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: dry\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: normal\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: oily\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: combination\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: sensitivity-high\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: sensitivity-low\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: no_sensitivity\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: male\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: female\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: cleanse\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: prepare\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: treat\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: targeted\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: care\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: moisturize\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: protect\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: day\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: night\n",
            "Ensemble (Logistic Regression + LightGBM) Results:\n",
            "                   Attribute  Accuracy        F1    Recall  Precision\n",
            "0          dark_pigmentation  0.963141  0.863905  0.901235   0.829545\n",
            "1                       acne  0.987179  0.937500  0.923077   0.952381\n",
            "2                eye_contour  0.983173  0.927336  0.924138   0.930556\n",
            "3                homogeneity  0.933494  0.763533  0.853503   0.690722\n",
            "4              lack_firmness  0.940705  0.843220  0.892377   0.799197\n",
            "5              lack_radiance  0.910256  0.872437  0.896956   0.849224\n",
            "6                      pores  0.943109  0.873890  0.897810   0.851211\n",
            "7                 fine_lines  0.919872  0.872774  0.890909   0.855362\n",
            "8        wrinkles_fine-lines  0.929487  0.901786  0.899777   0.903803\n",
            "9               eye-wrinkles  0.977564  0.889764  0.911290   0.869231\n",
            "10             undereye-bags  0.983173  0.909871  0.946429   0.876033\n",
            "11                   generic  0.788462  0.839220  0.936141   0.760486\n",
            "12                     18-34  0.821314  0.854723  0.913649   0.802938\n",
            "13                     35-54  0.846154  0.826087  0.853933   0.800000\n",
            "14                     55-99  0.881410  0.779762  0.823899   0.740113\n",
            "15                       dry  0.893429  0.839178  0.898964   0.786848\n",
            "16                    normal  0.814103  0.763747  0.842697   0.698324\n",
            "17                      oily  0.925481  0.835979  0.855596   0.817241\n",
            "18               combination  0.789263  0.718114  0.795724   0.654297\n",
            "19          sensitivity-high  0.927885  0.788732  0.852792   0.733624\n",
            "20           sensitivity-low  0.810096  0.384416  0.560606   0.292490\n",
            "21            no_sensitivity  0.794872  0.861322  0.944181   0.791833\n",
            "22                      male  0.980769  0.916084  0.929078   0.903448\n",
            "23                    female  0.827724  0.878187  0.905374   0.852585\n",
            "24                   cleanse  0.954327  0.884381  0.919831   0.851562\n",
            "25                   prepare  0.914263  0.783838  0.832618   0.740458\n",
            "26                     treat  0.881410  0.874576  0.894281   0.855721\n",
            "27                  targeted  0.899840  0.859076  0.898585   0.822894\n",
            "28                      care  0.766026  0.776417  0.866667   0.703190\n",
            "29                moisturize  0.894231  0.919903  0.941615   0.899170\n",
            "30                   protect  0.885417  0.820126  0.821159   0.819095\n",
            "31                       day  0.834135  0.896136  0.931178   0.863636\n",
            "32                     night  0.839744  0.866131  0.928264   0.811794\n",
            "Average              Average  0.892167  0.837035  0.881343   0.800273\n",
            "\n",
            "Micro Average:\n",
            "Precision: 0.80, Recall: 0.89, F1: 0.84\n",
            "\n",
            "Macro Average:\n",
            "Precision: 0.80, Recall: 0.88, F1: 0.84\n",
            "\n",
            "Weighted Average:\n",
            "Precision: 0.81, Recall: 0.89, F1: 0.85\n",
            "\n",
            "Samples Average:\n",
            "Precision: 0.79, Recall: 0.89, F1: 0.83\n",
            "\n",
            "Total Carbon Emissions: 0.0011 kg CO2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regression + LightGBM + Random**"
      ],
      "metadata": {
        "id": "e1jl4wqefQl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import lightgbm as lgb\n",
        "from codecarbon import EmissionsTracker\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Suppress scikit-learn deprecation warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Disable CodeCarbon logs\n",
        "logging.getLogger(\"codecarbon\").setLevel(logging.WARNING)\n",
        "\n",
        "# Delete the lock file if it exists\n",
        "lock_file = \"/tmp/.codecarbon.lock\"\n",
        "if os.path.exists(lock_file):\n",
        "    os.remove(lock_file)\n",
        "\n",
        "# Initialize CodeCarbon tracker (allow multiple runs)\n",
        "tracker = EmissionsTracker(log_level=\"error\", allow_multiple_runs=True)  # Suppress all logs except errors\n",
        "tracker.start()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/anthropic.claude-3-5-sonnet Full.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Enhanced Text Cleaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # Remove stopwords and lemmatize\n",
        "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df[\"text_raw\"] = df[\"text_raw\"].apply(clean_text)\n",
        "\n",
        "# Features: 'text_raw' column\n",
        "X = df[\"text_raw\"]\n",
        "\n",
        "# Labels: Binary attributes (columns 1 to 33)\n",
        "binary_columns = df.columns[1:34]  # Assuming columns 1 to 33 are binary attributes\n",
        "y = df[binary_columns]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data using TF-IDF with character n-grams\n",
        "vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 2))  # Reduced max_features\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression + LightGBM + Random Forest + XGBoost ensemble for each binary attribute\n",
        "results_ensemble = []\n",
        "y_pred_all = []  # To store predictions for all attributes\n",
        "\n",
        "for i, col in enumerate(binary_columns):\n",
        "    print(f\"Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: {col}\")\n",
        "\n",
        "    # Calculate class weights for imbalanced data\n",
        "    class_weights = {0: 1, 1: len(y_train[col]) / sum(y_train[col])}  # Higher weight for minority class\n",
        "\n",
        "    # Define Logistic Regression model with hyperparameter tuning\n",
        "    lr_model = LogisticRegression(\n",
        "        class_weight=\"balanced\",  # Handle imbalanced classes\n",
        "        max_iter=200,           # Reduced iterations\n",
        "        n_jobs=-1,               # Use all cores\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Define LightGBM model with hyperparameter tuning\n",
        "    lgb_model = lgb.LGBMClassifier(\n",
        "        n_estimators=50,  # Reduced number of trees\n",
        "        learning_rate=0.05,  # Lower learning rate for better generalization\n",
        "        max_depth=2,        # Reduced depth to prevent overfitting\n",
        "        random_state=42,\n",
        "        n_jobs=-1,          # Use all available CPU cores\n",
        "        class_weight=class_weights,  # Handle class imbalance\n",
        "        verbosity=-1,  # Suppress LightGBM warnings\n",
        "        subsample=0.8,  # Subsample to reduce computation\n",
        "        colsample_bytree=0.8  # Feature subsampling to reduce computation\n",
        "    )\n",
        "\n",
        "    # Define Random Forest model with hyperparameter tuning\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators=50,  # Reduced number of trees\n",
        "        max_depth=2,      # Reduced depth to prevent overfitting\n",
        "        class_weight=class_weights,  # Handle class imbalance\n",
        "        random_state=42,\n",
        "        n_jobs=-1         # Use all available CPU cores\n",
        "    )\n",
        "\n",
        "    # Define XGBoost model with hyperparameter tuning\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=50,  # Reduced number of trees\n",
        "        learning_rate=0.05,  # Lower learning rate for better generalization\n",
        "        max_depth=2,      # Reduced depth to prevent overfitting\n",
        "        random_state=42,\n",
        "        n_jobs=-1,        # Use all available CPU cores\n",
        "        scale_pos_weight=len(y_train[col]) / sum(y_train[col]),  # Handle class imbalance\n",
        "        subsample=0.8,    # Subsample to reduce computation\n",
        "        colsample_bytree=0.8  # Feature subsampling to reduce computation\n",
        "    )\n",
        "\n",
        "    # Create Voting Classifier (ensemble of Logistic Regression, LightGBM, Random Forest, and XGBoost)\n",
        "    ensemble_model = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('lr', lr_model),\n",
        "            ('lgb', lgb_model),\n",
        "            ('rf', rf_model),\n",
        "            ('xgb', xgb_model)\n",
        "        ],\n",
        "        voting='soft',  # Use soft voting for probabilistic predictions\n",
        "        n_jobs=-1       # Use all cores\n",
        "    )\n",
        "\n",
        "    # Train the ensemble model\n",
        "    ensemble_model.fit(X_train_tfidf, y_train[col])\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = ensemble_model.predict(X_test_tfidf)\n",
        "    y_pred_all.append(y_pred)  # Store predictions\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test[col], y_pred)\n",
        "    f1 = f1_score(y_test[col], y_pred, average='binary')\n",
        "    recall = recall_score(y_test[col], y_pred, average='binary')\n",
        "    precision = precision_score(y_test[col], y_pred, average='binary')\n",
        "\n",
        "    results_ensemble.append([col, accuracy, f1, recall, precision])\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_ensemble_df = pd.DataFrame(results_ensemble, columns=[\"Attribute\", \"Accuracy\", \"F1\", \"Recall\", \"Precision\"])\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_accuracy_ensemble = results_ensemble_df[\"Accuracy\"].mean()\n",
        "avg_f1_ensemble = results_ensemble_df[\"F1\"].mean()\n",
        "avg_recall_ensemble = results_ensemble_df[\"Recall\"].mean()\n",
        "avg_precision_ensemble = results_ensemble_df[\"Precision\"].mean()\n",
        "\n",
        "# Add average row to the results table\n",
        "results_ensemble_df.loc[\"Average\"] = [\"Average\", avg_accuracy_ensemble, avg_f1_ensemble, avg_recall_ensemble, avg_precision_ensemble]\n",
        "\n",
        "# Display results\n",
        "print(\"Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) Results:\")\n",
        "print(results_ensemble_df)\n",
        "\n",
        "# Convert predictions to a 2D array (num_samples, num_attributes)\n",
        "y_pred_all = np.array(y_pred_all).T\n",
        "\n",
        "# Generate classification report for micro, macro, weighted, and samples averages\n",
        "classification_report_result = classification_report(\n",
        "    y_test, y_pred_all, target_names=binary_columns, output_dict=True\n",
        ")\n",
        "\n",
        "# Extract micro, macro, weighted, and samples averages\n",
        "micro_avg = classification_report_result['micro avg']\n",
        "macro_avg = classification_report_result['macro avg']\n",
        "weighted_avg = classification_report_result['weighted avg']\n",
        "samples_avg = classification_report_result['samples avg']\n",
        "\n",
        "# Display the averages\n",
        "print(\"\\nMicro Average:\")\n",
        "print(f\"Precision: {micro_avg['precision']:.2f}, Recall: {micro_avg['recall']:.2f}, F1: {micro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nMacro Average:\")\n",
        "print(f\"Precision: {macro_avg['precision']:.2f}, Recall: {macro_avg['recall']:.2f}, F1: {macro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nWeighted Average:\")\n",
        "print(f\"Precision: {weighted_avg['precision']:.2f}, Recall: {weighted_avg['recall']:.2f}, F1: {weighted_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nSamples Average:\")\n",
        "print(f\"Precision: {samples_avg['precision']:.2f}, Recall: {samples_avg['recall']:.2f}, F1: {samples_avg['f1-score']:.2f}\")\n",
        "\n",
        "# Stop the CodeCarbon tracker and get the emissions\n",
        "emissions = tracker.stop()\n",
        "if emissions is None:\n",
        "    emissions = 0.0  # Default value if tracker fails\n",
        "\n",
        "print(f\"\\nTotal Carbon Emissions: {emissions:.4f} kg CO2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ps2WpFsYyyq",
        "outputId": "f9e26b60-6bd3-4c5d-d614-4523a2208322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[codecarbon WARNING @ 18:07:40] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: dark_pigmentation\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: acne\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: eye_contour\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: homogeneity\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: lack_firmness\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: lack_radiance\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: pores\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: fine_lines\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: wrinkles_fine-lines\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: eye-wrinkles\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: undereye-bags\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: generic\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: 18-34\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: 35-54\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: 55-99\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: dry\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: normal\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: oily\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: combination\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: sensitivity-high\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: sensitivity-low\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: no_sensitivity\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: male\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: female\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: cleanse\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: prepare\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: treat\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: targeted\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: care\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: moisturize\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: protect\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: day\n",
            "Training Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) for attribute: night\n",
            "Ensemble (Logistic Regression + LightGBM + Random Forest + XGBoost) Results:\n",
            "                   Attribute  Accuracy        F1    Recall  Precision\n",
            "0          dark_pigmentation  0.935897  0.772727  0.839506   0.715789\n",
            "1                       acne  0.970353  0.847737  0.792308   0.911504\n",
            "2                eye_contour  0.962340  0.856269  0.965517   0.769231\n",
            "3                homogeneity  0.902244  0.691919  0.872611   0.573222\n",
            "4              lack_firmness  0.918269  0.794355  0.883408   0.721612\n",
            "5              lack_radiance  0.860577  0.811688  0.878220   0.754527\n",
            "6                      pores  0.931891  0.847943  0.864964   0.831579\n",
            "7                 fine_lines  0.908654  0.852332  0.854545   0.850129\n",
            "8        wrinkles_fine-lines  0.916667  0.884187  0.884187   0.884187\n",
            "9               eye-wrinkles  0.964744  0.841727  0.943548   0.759740\n",
            "10             undereye-bags  0.973558  0.864198  0.937500   0.801527\n",
            "11                   generic  0.673878  0.782236  0.993207   0.645190\n",
            "12                     18-34  0.724359  0.802978  0.976323   0.681907\n",
            "13                     35-54  0.767628  0.766881  0.893258   0.671831\n",
            "14                     55-99  0.854167  0.741477  0.820755   0.676166\n",
            "15                       dry  0.845353  0.778923  0.880829   0.698152\n",
            "16                    normal  0.728365  0.699734  0.887640   0.577485\n",
            "17                      oily  0.905449  0.800000  0.851986   0.753994\n",
            "18               combination  0.688301  0.658472  0.890736   0.522284\n",
            "19          sensitivity-high  0.911859  0.746544  0.822335   0.683544\n",
            "20           sensitivity-low  0.700321  0.332143  0.704545   0.217290\n",
            "21            no_sensitivity  0.748397  0.839468  0.975059   0.736984\n",
            "22                      male  0.976763  0.894545  0.872340   0.917910\n",
            "23                    female  0.748397  0.844708  0.997664   0.732419\n",
            "24                   cleanse  0.939103  0.849206  0.902954   0.801498\n",
            "25                   prepare  0.876603  0.712687  0.819742   0.630363\n",
            "26                     treat  0.784455  0.801769  0.942808   0.697436\n",
            "27                  targeted  0.860577  0.815678  0.908019   0.740385\n",
            "28                      care  0.701122  0.750835  0.960684   0.616228\n",
            "29                moisturize  0.831731  0.882155  0.976398   0.804504\n",
            "30                   protect  0.889423  0.820779  0.795970   0.847185\n",
            "31                       day  0.798077  0.883441  0.995829   0.793849\n",
            "32                     night  0.725962  0.800931  0.987088   0.673849\n",
            "Average              Average  0.846227  0.790020  0.896136   0.717985\n",
            "\n",
            "Micro Average:\n",
            "Precision: 0.70, Recall: 0.92, F1: 0.80\n",
            "\n",
            "Macro Average:\n",
            "Precision: 0.72, Recall: 0.90, F1: 0.79\n",
            "\n",
            "Weighted Average:\n",
            "Precision: 0.72, Recall: 0.92, F1: 0.80\n",
            "\n",
            "Samples Average:\n",
            "Precision: 0.69, Recall: 0.93, F1: 0.78\n",
            "\n",
            "Total Carbon Emissions: 0.0009 kg CO2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Logistic Regression ***"
      ],
      "metadata": {
        "id": "MuLY9rVBfkHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from codecarbon import EmissionsTracker\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress scikit-learn deprecation warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Disable CodeCarbon logs\n",
        "logging.getLogger(\"codecarbon\").setLevel(logging.WARNING)\n",
        "\n",
        "# Delete the lock file if it exists\n",
        "lock_file = \"/tmp/.codecarbon.lock\"\n",
        "if os.path.exists(lock_file):\n",
        "    os.remove(lock_file)\n",
        "\n",
        "# Initialize CodeCarbon tracker (allow multiple runs)\n",
        "tracker = EmissionsTracker(log_level=\"error\", allow_multiple_runs=True)  # Suppress all logs except errors\n",
        "tracker.start()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/anthropic.claude-3-5-sonnet Full.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Clean the 'text_raw' column\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df[\"text_raw\"] = df[\"text_raw\"].apply(clean_text)\n",
        "\n",
        "# Features: 'text_raw' column\n",
        "X = df[\"text_raw\"]\n",
        "\n",
        "# Labels: Binary attributes (columns 1 to 33)\n",
        "binary_columns = df.columns[1:34]  # Assuming columns 1 to 33 are binary attributes\n",
        "y = df[binary_columns]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features for efficiency\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model for each binary attribute\n",
        "results_lr = []\n",
        "y_pred_all = []  # To store predictions for all attributes\n",
        "\n",
        "for i, col in enumerate(binary_columns):\n",
        "    print(f\"Training Logistic Regression for attribute: {col}\")\n",
        "\n",
        "    # Train Logistic Regression model\n",
        "    model = LogisticRegression(\n",
        "        class_weight=\"balanced\",  # Handle imbalanced classes\n",
        "        max_iter=1000,           # Increase iterations for convergence\n",
        "        n_jobs=-1,               # Use all cores\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train_tfidf, y_train[col])\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    y_pred_all.append(y_pred)  # Store predictions\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test[col], y_pred)\n",
        "    f1 = f1_score(y_test[col], y_pred, average='binary')\n",
        "    recall = recall_score(y_test[col], y_pred, average='binary')\n",
        "    precision = precision_score(y_test[col], y_pred, average='binary')\n",
        "\n",
        "    results_lr.append([col, accuracy, f1, recall, precision])\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_lr_df = pd.DataFrame(results_lr, columns=[\"Attribute\", \"Accuracy\", \"F1\", \"Recall\", \"Precision\"])\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_accuracy_lr = results_lr_df[\"Accuracy\"].mean()\n",
        "avg_f1_lr = results_lr_df[\"F1\"].mean()\n",
        "avg_recall_lr = results_lr_df[\"Recall\"].mean()\n",
        "avg_precision_lr = results_lr_df[\"Precision\"].mean()\n",
        "\n",
        "# Add average row to the results table\n",
        "results_lr_df.loc[\"Average\"] = [\"Average\", avg_accuracy_lr, avg_f1_lr, avg_recall_lr, avg_precision_lr]\n",
        "\n",
        "# Display results\n",
        "print(\"Logistic Regression Results:\")\n",
        "print(results_lr_df)\n",
        "\n",
        "# Convert predictions to a 2D array (num_samples, num_attributes)\n",
        "y_pred_all = np.array(y_pred_all).T\n",
        "\n",
        "# Generate classification report for micro, macro, weighted, and samples averages\n",
        "classification_report_result = classification_report(\n",
        "    y_test, y_pred_all, target_names=binary_columns, output_dict=True\n",
        ")\n",
        "\n",
        "# Extract micro, macro, weighted, and samples averages\n",
        "micro_avg = classification_report_result['micro avg']\n",
        "macro_avg = classification_report_result['macro avg']\n",
        "weighted_avg = classification_report_result['weighted avg']\n",
        "samples_avg = classification_report_result['samples avg']\n",
        "\n",
        "# Display the averages\n",
        "print(\"\\nMicro Average:\")\n",
        "print(f\"Precision: {micro_avg['precision']:.2f}, Recall: {micro_avg['recall']:.2f}, F1: {micro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nMacro Average:\")\n",
        "print(f\"Precision: {macro_avg['precision']:.2f}, Recall: {macro_avg['recall']:.2f}, F1: {macro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nWeighted Average:\")\n",
        "print(f\"Precision: {weighted_avg['precision']:.2f}, Recall: {weighted_avg['recall']:.2f}, F1: {weighted_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nSamples Average:\")\n",
        "print(f\"Precision: {samples_avg['precision']:.2f}, Recall: {samples_avg['recall']:.2f}, F1: {samples_avg['f1-score']:.2f}\")\n",
        "\n",
        "# Stop the CodeCarbon tracker and get the emissions\n",
        "emissions = tracker.stop()\n",
        "if emissions is None:\n",
        "    emissions = 0.0  # Default value if tracker fails\n",
        "\n",
        "print(f\"\\nTotal Carbon Emissions: {emissions:.4f} kg CO2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foCf_GHIZ-Tr",
        "outputId": "0933400c-afd7-499f-d6fc-daeed05c64d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 19:05:38] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression for attribute: dark_pigmentation\n",
            "Training Logistic Regression for attribute: acne\n",
            "Training Logistic Regression for attribute: eye_contour\n",
            "Training Logistic Regression for attribute: homogeneity\n",
            "Training Logistic Regression for attribute: lack_firmness\n",
            "Training Logistic Regression for attribute: lack_radiance\n",
            "Training Logistic Regression for attribute: pores\n",
            "Training Logistic Regression for attribute: fine_lines\n",
            "Training Logistic Regression for attribute: wrinkles_fine-lines\n",
            "Training Logistic Regression for attribute: eye-wrinkles\n",
            "Training Logistic Regression for attribute: undereye-bags\n",
            "Training Logistic Regression for attribute: generic\n",
            "Training Logistic Regression for attribute: 18-34\n",
            "Training Logistic Regression for attribute: 35-54\n",
            "Training Logistic Regression for attribute: 55-99\n",
            "Training Logistic Regression for attribute: dry\n",
            "Training Logistic Regression for attribute: normal\n",
            "Training Logistic Regression for attribute: oily\n",
            "Training Logistic Regression for attribute: combination\n",
            "Training Logistic Regression for attribute: sensitivity-high\n",
            "Training Logistic Regression for attribute: sensitivity-low\n",
            "Training Logistic Regression for attribute: no_sensitivity\n",
            "Training Logistic Regression for attribute: male\n",
            "Training Logistic Regression for attribute: female\n",
            "Training Logistic Regression for attribute: cleanse\n",
            "Training Logistic Regression for attribute: prepare\n",
            "Training Logistic Regression for attribute: treat\n",
            "Training Logistic Regression for attribute: targeted\n",
            "Training Logistic Regression for attribute: care\n",
            "Training Logistic Regression for attribute: moisturize\n",
            "Training Logistic Regression for attribute: protect\n",
            "Training Logistic Regression for attribute: day\n",
            "Training Logistic Regression for attribute: night\n",
            "Logistic Regression Results:\n",
            "                   Attribute  Accuracy        F1    Recall  Precision\n",
            "0          dark_pigmentation  0.943109  0.797721  0.864198   0.740741\n",
            "1                       acne  0.955128  0.808219  0.907692   0.728395\n",
            "2                eye_contour  0.973558  0.881720  0.848276   0.917910\n",
            "3                homogeneity  0.885417  0.643392  0.821656   0.528689\n",
            "4              lack_firmness  0.918269  0.790123  0.860987   0.730038\n",
            "5              lack_radiance  0.866987  0.810502  0.831382   0.790646\n",
            "6                      pores  0.916667  0.811594  0.817518   0.805755\n",
            "7                 fine_lines  0.909455  0.856051  0.872727   0.840000\n",
            "8        wrinkles_fine-lines  0.913462  0.879464  0.877506   0.881432\n",
            "9               eye-wrinkles  0.975962  0.879032  0.879032   0.879032\n",
            "10             undereye-bags  0.980769  0.893805  0.901786   0.885965\n",
            "11                   generic  0.792468  0.825589  0.832880   0.818425\n",
            "12                     18-34  0.825321  0.848611  0.850975   0.846260\n",
            "13                     35-54  0.843750  0.818605  0.823970   0.813309\n",
            "14                     55-99  0.878205  0.779070  0.842767   0.724324\n",
            "15                       dry  0.873397  0.803483  0.836788   0.772727\n",
            "16                    normal  0.785256  0.722567  0.784270   0.669866\n",
            "17                      oily  0.893429  0.773424  0.819495   0.732258\n",
            "18               combination  0.774038  0.693478  0.757720   0.639279\n",
            "19          sensitivity-high  0.911058  0.748299  0.837563   0.676230\n",
            "20           sensitivity-low  0.793269  0.373786  0.583333   0.275000\n",
            "21            no_sensitivity  0.748397  0.808537  0.787411   0.830827\n",
            "22                      male  0.959135  0.823529  0.843972   0.804054\n",
            "23                    female  0.808494  0.853284  0.811916   0.899094\n",
            "24                   cleanse  0.956731  0.887967  0.902954   0.873469\n",
            "25                   prepare  0.906250  0.768317  0.832618   0.713235\n",
            "26                     treat  0.884615  0.871886  0.849220   0.895795\n",
            "27                  targeted  0.892628  0.848073  0.882075   0.816594\n",
            "28                      care  0.767628  0.765372  0.808547   0.726575\n",
            "29                moisturize  0.888622  0.911858  0.893168   0.931347\n",
            "30                   protect  0.866987  0.790932  0.790932   0.790932\n",
            "31                       day  0.783654  0.851485  0.807091   0.901048\n",
            "32                     night  0.826122  0.844221  0.843615   0.844828\n",
            "Average              Average  0.875704  0.801939  0.833516   0.779518\n",
            "\n",
            "Micro Average:\n",
            "Precision: 0.80, Recall: 0.83, F1: 0.81\n",
            "\n",
            "Macro Average:\n",
            "Precision: 0.78, Recall: 0.83, F1: 0.80\n",
            "\n",
            "Weighted Average:\n",
            "Precision: 0.81, Recall: 0.83, F1: 0.82\n",
            "\n",
            "Samples Average:\n",
            "Precision: 0.80, Recall: 0.82, F1: 0.79\n",
            "\n",
            "Total Carbon Emissions: 0.0001 kg CO2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LightGBM 2**"
      ],
      "metadata": {
        "id": "6ImdqLkgfsNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import lightgbm as lgb\n",
        "from codecarbon import EmissionsTracker\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress scikit-learn deprecation warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Disable CodeCarbon logs\n",
        "logging.getLogger(\"codecarbon\").setLevel(logging.WARNING)\n",
        "\n",
        "# Delete the lock file if it exists\n",
        "lock_file = \"/tmp/.codecarbon.lock\"\n",
        "if os.path.exists(lock_file):\n",
        "    os.remove(lock_file)\n",
        "\n",
        "# Initialize CodeCarbon tracker (allow multiple runs)\n",
        "tracker = EmissionsTracker(log_level=\"error\", allow_multiple_runs=True)  # Suppress all logs except errors\n",
        "tracker.start()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/anthropic.claude-3-5-sonnet Full.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Clean the 'text_raw' column\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df[\"text_raw\"] = df[\"text_raw\"].apply(clean_text)\n",
        "\n",
        "# Features: 'text_raw' column\n",
        "X = df[\"text_raw\"]\n",
        "\n",
        "# Labels: Binary attributes (columns 1 to 33)\n",
        "binary_columns = df.columns[1:34]  # Assuming columns 1 to 33 are binary attributes\n",
        "y = df[binary_columns]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data using TF-IDF with fewer features\n",
        "vectorizer = TfidfVectorizer(max_features=3000)  # Reduced to 3000 features for efficiency\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train LightGBM model for each binary attribute with optimized hyperparameters\n",
        "results_lgbm = []\n",
        "y_pred_all = []  # To store predictions for all attributes\n",
        "\n",
        "for i, col in enumerate(binary_columns):\n",
        "    print(f\"Training LightGBM for attribute: {col}\")\n",
        "\n",
        "    # Calculate class weights for imbalanced data\n",
        "    class_weights = {0: 1, 1: len(y_train[col]) / sum(y_train[col])}  # Higher weight for minority class\n",
        "\n",
        "    # Train LightGBM model with optimized hyperparameters\n",
        "    model = lgb.LGBMClassifier(\n",
        "        n_estimators=150,  # Reduced number of trees\n",
        "        learning_rate=0.1,  # Slightly higher learning rate for faster convergence\n",
        "        max_depth=5,        # Reduced depth to prevent overfitting\n",
        "        random_state=42,\n",
        "        n_jobs=-1,          # Use all available CPU cores\n",
        "        class_weight=class_weights,  # Handle class imbalance\n",
        "        verbosity=-1,  # Suppress LightGBM warnings\n",
        "        subsample=0.8,  # Subsample to reduce computation\n",
        "        colsample_bytree=0.8  # Feature subsampling to reduce computation\n",
        "    )\n",
        "\n",
        "    # Use early stopping with callbacks\n",
        "    callbacks = [\n",
        "        lgb.early_stopping(stopping_rounds=10, verbose=False),  # Early stopping\n",
        "        lgb.log_evaluation(period=0)  # Disable logging\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        X_train_tfidf,\n",
        "        y_train[col],\n",
        "        eval_set=[(X_test_tfidf, y_test[col])],\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    y_pred_all.append(y_pred)  # Store predictions\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test[col], y_pred)\n",
        "    f1 = f1_score(y_test[col], y_pred, average='binary')\n",
        "    recall = recall_score(y_test[col], y_pred, average='binary')\n",
        "    precision = precision_score(y_test[col], y_pred, average='binary')\n",
        "\n",
        "    results_lgbm.append([col, accuracy, f1, recall, precision])\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_lgbm_df = pd.DataFrame(results_lgbm, columns=[\"Attribute\", \"Accuracy\", \"F1\", \"Recall\", \"Precision\"])\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_accuracy_lgbm = results_lgbm_df[\"Accuracy\"].mean()\n",
        "avg_f1_lgbm = results_lgbm_df[\"F1\"].mean()\n",
        "avg_recall_lgbm = results_lgbm_df[\"Recall\"].mean()\n",
        "avg_precision_lgbm = results_lgbm_df[\"Precision\"].mean()\n",
        "\n",
        "# Add average row to the results table\n",
        "results_lgbm_df.loc[\"Average\"] = [\"Average\", avg_accuracy_lgbm, avg_f1_lgbm, avg_recall_lgbm, avg_precision_lgbm]\n",
        "\n",
        "# Display results\n",
        "print(\"LightGBM Results:\")\n",
        "print(results_lgbm_df)\n",
        "\n",
        "# Convert predictions to a 2D array (num_samples, num_attributes)\n",
        "y_pred_all = np.array(y_pred_all).T\n",
        "\n",
        "# Generate classification report for micro, macro, weighted, and samples averages\n",
        "classification_report_result = classification_report(\n",
        "    y_test, y_pred_all, target_names=binary_columns, output_dict=True\n",
        ")\n",
        "\n",
        "# Extract micro, macro, weighted, and samples averages\n",
        "micro_avg = classification_report_result['micro avg']\n",
        "macro_avg = classification_report_result['macro avg']\n",
        "weighted_avg = classification_report_result['weighted avg']\n",
        "samples_avg = classification_report_result['samples avg']\n",
        "\n",
        "# Display the averages\n",
        "print(\"\\nMicro Average:\")\n",
        "print(f\"Precision: {micro_avg['precision']:.2f}, Recall: {micro_avg['recall']:.2f}, F1: {micro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nMacro Average:\")\n",
        "print(f\"Precision: {macro_avg['precision']:.2f}, Recall: {macro_avg['recall']:.2f}, F1: {macro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nWeighted Average:\")\n",
        "print(f\"Precision: {weighted_avg['precision']:.2f}, Recall: {weighted_avg['recall']:.2f}, F1: {weighted_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nSamples Average:\")\n",
        "print(f\"Precision: {samples_avg['precision']:.2f}, Recall: {samples_avg['recall']:.2f}, F1: {samples_avg['f1-score']:.2f}\")\n",
        "\n",
        "# Stop the CodeCarbon tracker and get the emissions\n",
        "emissions = tracker.stop()\n",
        "if emissions is None:\n",
        "    emissions = 0.0  # Default value if tracker fails\n",
        "\n",
        "print(f\"\\nTotal Carbon Emissions: {emissions:.4f} kg CO2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XuC4B55ezPf",
        "outputId": "bf37c984-ba94-4b76-f2b6-cf0f9b671d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 18:38:37] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LightGBM for attribute: dark_pigmentation\n",
            "Training LightGBM for attribute: acne\n",
            "Training LightGBM for attribute: eye_contour\n",
            "Training LightGBM for attribute: homogeneity\n",
            "Training LightGBM for attribute: lack_firmness\n",
            "Training LightGBM for attribute: lack_radiance\n",
            "Training LightGBM for attribute: pores\n",
            "Training LightGBM for attribute: fine_lines\n",
            "Training LightGBM for attribute: wrinkles_fine-lines\n",
            "Training LightGBM for attribute: eye-wrinkles\n",
            "Training LightGBM for attribute: undereye-bags\n",
            "Training LightGBM for attribute: generic\n",
            "Training LightGBM for attribute: 18-34\n",
            "Training LightGBM for attribute: 35-54\n",
            "Training LightGBM for attribute: 55-99\n",
            "Training LightGBM for attribute: dry\n",
            "Training LightGBM for attribute: normal\n",
            "Training LightGBM for attribute: oily\n",
            "Training LightGBM for attribute: combination\n",
            "Training LightGBM for attribute: sensitivity-high\n",
            "Training LightGBM for attribute: sensitivity-low\n",
            "Training LightGBM for attribute: no_sensitivity\n",
            "Training LightGBM for attribute: male\n",
            "Training LightGBM for attribute: female\n",
            "Training LightGBM for attribute: cleanse\n",
            "Training LightGBM for attribute: prepare\n",
            "Training LightGBM for attribute: treat\n",
            "Training LightGBM for attribute: targeted\n",
            "Training LightGBM for attribute: care\n",
            "Training LightGBM for attribute: moisturize\n",
            "Training LightGBM for attribute: protect\n",
            "Training LightGBM for attribute: day\n",
            "Training LightGBM for attribute: night\n",
            "LightGBM Results:\n",
            "                   Attribute  Accuracy        F1    Recall  Precision\n",
            "0          dark_pigmentation  0.965545  0.871642  0.901235   0.843931\n",
            "1                       acne  0.985577  0.930769  0.930769   0.930769\n",
            "2                eye_contour  0.982372  0.923077  0.910345   0.936170\n",
            "3                homogeneity  0.938301  0.781870  0.878981   0.704082\n",
            "4              lack_firmness  0.939904  0.838013  0.869955   0.808333\n",
            "5              lack_radiance  0.914263  0.875437  0.880562   0.870370\n",
            "6                      pores  0.927885  0.837545  0.846715   0.828571\n",
            "7                 fine_lines  0.908654  0.853846  0.864935   0.843038\n",
            "8        wrinkles_fine-lines  0.927885  0.898649  0.888641   0.908884\n",
            "9               eye-wrinkles  0.979167  0.896000  0.903226   0.888889\n",
            "10             undereye-bags  0.983173  0.910638  0.955357   0.869919\n",
            "11                   generic  0.751603  0.819137  0.953804   0.717791\n",
            "12                     18-34  0.793269  0.840149  0.944290   0.756696\n",
            "13                     35-54  0.819712  0.804857  0.868914   0.749596\n",
            "14                     55-99  0.875801  0.770370  0.817610   0.728291\n",
            "15                       dry  0.869391  0.801944  0.854922   0.755149\n",
            "16                    normal  0.794872  0.744511  0.838202   0.669659\n",
            "17                      oily  0.918269  0.821053  0.844765   0.798635\n",
            "18               combination  0.778045  0.711759  0.812352   0.633333\n",
            "19          sensitivity-high  0.926282  0.780952  0.832487   0.735426\n",
            "20           sensitivity-low  0.793269  0.317460  0.454545   0.243902\n",
            "21            no_sensitivity  0.773237  0.850974  0.959620   0.764428\n",
            "22                      male  0.979968  0.913495  0.936170   0.891892\n",
            "23                    female  0.820513  0.879050  0.950935   0.817269\n",
            "24                   cleanse  0.951122  0.876768  0.915612   0.841085\n",
            "25                   prepare  0.909455  0.768916  0.806867   0.734375\n",
            "26                     treat  0.854968  0.851029  0.896014   0.810345\n",
            "27                  targeted  0.892628  0.851111  0.903302   0.804622\n",
            "28                      care  0.753205  0.775510  0.909402   0.675985\n",
            "29                moisturize  0.893429  0.919831  0.947826   0.893443\n",
            "30                   protect  0.883013  0.816583  0.818640   0.814536\n",
            "31                       day  0.823718  0.895238  0.980188   0.823839\n",
            "32                     night  0.794071  0.836201  0.941176   0.752294\n",
            "Average              Average  0.881896  0.826193  0.879344   0.783198\n",
            "\n",
            "Micro Average:\n",
            "Precision: 0.78, Recall: 0.90, F1: 0.83\n",
            "\n",
            "Macro Average:\n",
            "Precision: 0.78, Recall: 0.88, F1: 0.83\n",
            "\n",
            "Weighted Average:\n",
            "Precision: 0.78, Recall: 0.90, F1: 0.84\n",
            "\n",
            "Samples Average:\n",
            "Precision: 0.76, Recall: 0.91, F1: 0.82\n",
            "\n",
            "Total Carbon Emissions: 0.0008 kg CO2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SVM**"
      ],
      "metadata": {
        "id": "vdo5hFSqf49g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from codecarbon import EmissionsTracker\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings and logs\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "logging.getLogger(\"codecarbon\").setLevel(logging.WARNING)\n",
        "\n",
        "# Remove CodeCarbon lock file if it exists\n",
        "lock_file = \"/tmp/.codecarbon.lock\"\n",
        "if os.path.exists(lock_file):\n",
        "    os.remove(lock_file)\n",
        "\n",
        "# Start CodeCarbon tracker\n",
        "tracker = EmissionsTracker(log_level=\"error\", allow_multiple_runs=True)\n",
        "tracker.start()\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/anthropic.claude-3-5-sonnet Full.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df[\"text_raw\"] = df[\"text_raw\"].apply(clean_text)\n",
        "\n",
        "# Define features and labels\n",
        "X = df[\"text_raw\"]\n",
        "binary_columns = df.columns[1:34]\n",
        "y = df[binary_columns]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TF-IDF Vectorization with feature selection\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Limit features for efficiency\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# SVM training with optimizations\n",
        "results_svm = []\n",
        "y_pred_all = []\n",
        "\n",
        "for col in binary_columns:\n",
        "    print(f\"Training SVM for attribute: {col}\")\n",
        "\n",
        "    # Train SVM model\n",
        "    model = SVC(kernel=\"linear\", class_weight=\"balanced\", probability=True)\n",
        "    model.fit(X_train_tfidf, y_train[col])\n",
        "\n",
        "    # Predict and store results\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    y_pred_all.append(y_pred)\n",
        "\n",
        "    # Compute metrics\n",
        "    results_svm.append([\n",
        "        col,\n",
        "        accuracy_score(y_test[col], y_pred),\n",
        "        f1_score(y_test[col], y_pred, average='binary'),\n",
        "        recall_score(y_test[col], y_pred, average='binary'),\n",
        "        precision_score(y_test[col], y_pred, average='binary')\n",
        "    ])\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_svm_df = pd.DataFrame(results_svm, columns=[\"Attribute\", \"Accuracy\", \"F1\", \"Recall\", \"Precision\"])\n",
        "\n",
        "# Compute and append averages\n",
        "avg_metrics = results_svm_df.mean(numeric_only=True)\n",
        "results_svm_df.loc[\"Average\"] = [\"Average\"] + avg_metrics.tolist()\n",
        "\n",
        "# Display results\n",
        "print(\"Optimized SVM Results:\")\n",
        "print(results_svm_df)\n",
        "\n",
        "# Convert predictions to a structured array\n",
        "y_pred_all = np.array(y_pred_all).T\n",
        "\n",
        "# Generate classification report\n",
        "classification_report_result = classification_report(y_test, y_pred_all, target_names=binary_columns, output_dict=True)\n",
        "\n",
        "# Extract key averages\n",
        "for avg_type in [\"macro avg\", \"weighted avg\"]:\n",
        "    print(f\"\\n{avg_type.capitalize()}:\")\n",
        "    print(f\"Precision: {classification_report_result[avg_type]['precision']:.2f}, \"\n",
        "          f\"Recall: {classification_report_result[avg_type]['recall']:.2f}, \"\n",
        "          f\"F1: {classification_report_result[avg_type]['f1-score']:.2f}\")\n",
        "\n",
        "# Stop emissions tracker\n",
        "emissions = tracker.stop() or 0.0\n",
        "print(f\"\\nTotal Carbon Emissions: {emissions:.4f} kg CO2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrhBTFRGezSS",
        "outputId": "37dce3c1-725c-4ca2-edf7-f0991bdde7ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 18:40:04] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM for attribute: dark_pigmentation\n",
            "Training SVM for attribute: acne\n",
            "Training SVM for attribute: eye_contour\n",
            "Training SVM for attribute: homogeneity\n",
            "Training SVM for attribute: lack_firmness\n",
            "Training SVM for attribute: lack_radiance\n",
            "Training SVM for attribute: pores\n",
            "Training SVM for attribute: fine_lines\n",
            "Training SVM for attribute: wrinkles_fine-lines\n",
            "Training SVM for attribute: eye-wrinkles\n",
            "Training SVM for attribute: undereye-bags\n",
            "Training SVM for attribute: generic\n",
            "Training SVM for attribute: 18-34\n",
            "Training SVM for attribute: 35-54\n",
            "Training SVM for attribute: 55-99\n",
            "Training SVM for attribute: dry\n",
            "Training SVM for attribute: normal\n",
            "Training SVM for attribute: oily\n",
            "Training SVM for attribute: combination\n",
            "Training SVM for attribute: sensitivity-high\n",
            "Training SVM for attribute: sensitivity-low\n",
            "Training SVM for attribute: no_sensitivity\n",
            "Training SVM for attribute: male\n",
            "Training SVM for attribute: female\n",
            "Training SVM for attribute: cleanse\n",
            "Training SVM for attribute: prepare\n",
            "Training SVM for attribute: treat\n",
            "Training SVM for attribute: targeted\n",
            "Training SVM for attribute: care\n",
            "Training SVM for attribute: moisturize\n",
            "Training SVM for attribute: protect\n",
            "Training SVM for attribute: day\n",
            "Training SVM for attribute: night\n",
            "Optimized SVM Results:\n",
            "                   Attribute  Accuracy        F1    Recall  Precision\n",
            "0          dark_pigmentation  0.947115  0.808140  0.858025   0.763736\n",
            "1                       acne  0.968750  0.857143  0.900000   0.818182\n",
            "2                eye_contour  0.973558  0.885017  0.875862   0.894366\n",
            "3                homogeneity  0.908654  0.690217  0.808917   0.601896\n",
            "4              lack_firmness  0.929487  0.814346  0.865471   0.768924\n",
            "5              lack_radiance  0.879006  0.825029  0.833724   0.816514\n",
            "6                      pores  0.914263  0.805100  0.806569   0.803636\n",
            "7                 fine_lines  0.915064  0.864103  0.875325   0.853165\n",
            "8        wrinkles_fine-lines  0.927083  0.897407  0.886414   0.908676\n",
            "9               eye-wrinkles  0.975160  0.874494  0.870968   0.878049\n",
            "10             undereye-bags  0.977564  0.878261  0.901786   0.855932\n",
            "11                   generic  0.799679  0.829235  0.824728   0.833791\n",
            "12                     18-34  0.819712  0.844291  0.849582   0.839065\n",
            "13                     35-54  0.837340  0.812558  0.823970   0.801457\n",
            "14                     55-99  0.882212  0.782222  0.830189   0.739496\n",
            "15                       dry  0.880609  0.816728  0.860104   0.777518\n",
            "16                    normal  0.787660  0.724818  0.784270   0.673745\n",
            "17                      oily  0.887821  0.759450  0.797834   0.724590\n",
            "18               combination  0.751603  0.663774  0.726841   0.610778\n",
            "19          sensitivity-high  0.910256  0.740741  0.812183   0.680851\n",
            "20           sensitivity-low  0.803686  0.376590  0.560606   0.283525\n",
            "21            no_sensitivity  0.748397  0.806412  0.776722   0.838462\n",
            "22                      male  0.963942  0.844291  0.865248   0.824324\n",
            "23                    female  0.817308  0.861314  0.827103   0.898477\n",
            "24                   cleanse  0.951122  0.872651  0.881857   0.863636\n",
            "25                   prepare  0.903846  0.755102  0.793991   0.719844\n",
            "26                     treat  0.879006  0.869037  0.868284   0.869792\n",
            "27                  targeted  0.891026  0.847191  0.889151   0.809013\n",
            "28                      care  0.766827  0.765889  0.813675   0.723404\n",
            "29                moisturize  0.890224  0.913126  0.894410   0.932642\n",
            "30                   protect  0.880609  0.812579  0.813602   0.811558\n",
            "31                       day  0.782051  0.850385  0.806048   0.899884\n",
            "32                     night  0.829327  0.846873  0.845050   0.848703\n",
            "Average              Average  0.878181  0.805894  0.831167   0.786898\n",
            "\n",
            "Macro avg:\n",
            "Precision: 0.79, Recall: 0.83, F1: 0.81\n",
            "\n",
            "Weighted avg:\n",
            "Precision: 0.81, Recall: 0.83, F1: 0.82\n",
            "\n",
            "Total Carbon Emissions: 0.0151 kg CO2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Boosting**"
      ],
      "metadata": {
        "id": "YaCxoLYdgcX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from codecarbon import EmissionsTracker\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Suppress scikit-learn deprecation warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Disable CodeCarbon logs\n",
        "logging.getLogger(\"codecarbon\").setLevel(logging.WARNING)\n",
        "\n",
        "# Delete the lock file if it exists\n",
        "lock_file = \"/tmp/.codecarbon.lock\"\n",
        "if os.path.exists(lock_file):\n",
        "    os.remove(lock_file)\n",
        "\n",
        "# Initialize CodeCarbon tracker (allow multiple runs)\n",
        "tracker = EmissionsTracker(log_level=\"error\", allow_multiple_runs=True)  # Suppress all logs except errors\n",
        "tracker.start()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/anthropic.claude-3-5-sonnet Full.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Enhanced Text Cleaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # Remove stopwords and lemmatize\n",
        "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df[\"text_raw\"] = df[\"text_raw\"].apply(clean_text)\n",
        "\n",
        "# Features: 'text_raw' column\n",
        "X = df[\"text_raw\"]\n",
        "\n",
        "# Labels: Binary attributes (columns 1 to 33)\n",
        "binary_columns = df.columns[1:34]  # Assuming columns 1 to 33 are binary attributes\n",
        "y = df[binary_columns]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data using TF-IDF with character n-grams\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))  # Use character n-grams\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Handle imbalanced data using SMOTE (apply SMOTE on each label separately)\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Initialize lists to store resampled training data and labels for each column\n",
        "X_train_res_list = []\n",
        "y_train_res_list = []\n",
        "\n",
        "# Apply SMOTE to each binary target column\n",
        "for col in binary_columns:\n",
        "    X_res, y_res = smote.fit_resample(X_train_tfidf, y_train[col])\n",
        "    X_train_res_list.append(X_res)\n",
        "    y_train_res_list.append(y_res)\n",
        "\n",
        "# Train Gradient Boosting model for each binary attribute\n",
        "results_gb = []\n",
        "y_pred_all = []  # To store predictions for all attributes\n",
        "\n",
        "for i, col in enumerate(binary_columns):\n",
        "    print(f\"Training Gradient Boosting for attribute: {col}\")\n",
        "\n",
        "    # Train Gradient Boosting model on resampled data\n",
        "    model = GradientBoostingClassifier(\n",
        "        n_estimators=200,  # Increase number of trees\n",
        "        learning_rate=0.05,  # Lower learning rate for better generalization\n",
        "        max_depth=5,        # Limit depth to prevent overfitting\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(X_train_res_list[i], y_train_res_list[i])\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    y_pred_all.append(y_pred)  # Store predictions\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test[col], y_pred)\n",
        "    f1 = f1_score(y_test[col], y_pred, average='binary')\n",
        "    recall = recall_score(y_test[col], y_pred, average='binary')\n",
        "    precision = precision_score(y_test[col], y_pred, average='binary')\n",
        "\n",
        "    results_gb.append([col, accuracy, f1, recall, precision])\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_gb_df = pd.DataFrame(results_gb, columns=[\"Attribute\", \"Accuracy\", \"F1\", \"Recall\", \"Precision\"])\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_accuracy_gb = results_gb_df[\"Accuracy\"].mean()\n",
        "avg_f1_gb = results_gb_df[\"F1\"].mean()\n",
        "avg_recall_gb = results_gb_df[\"Recall\"].mean()\n",
        "avg_precision_gb = results_gb_df[\"Precision\"].mean()\n",
        "\n",
        "# Add average row to the results table\n",
        "results_gb_df.loc[\"Average\"] = [\"Average\", avg_accuracy_gb, avg_f1_gb, avg_recall_gb, avg_precision_gb]\n",
        "\n",
        "# Display results\n",
        "print(\"Gradient Boosting Results:\")\n",
        "print(results_gb_df)\n",
        "\n",
        "# Convert predictions to a 2D array (num_samples, num_attributes)\n",
        "y_pred_all = np.array(y_pred_all).T\n",
        "\n",
        "# Generate classification report for micro, macro, weighted, and samples averages\n",
        "classification_report_result = classification_report(\n",
        "    y_test, y_pred_all, target_names=binary_columns, output_dict=True\n",
        ")\n",
        "\n",
        "# Extract micro, macro, weighted, and samples averages\n",
        "micro_avg = classification_report_result['micro avg']\n",
        "macro_avg = classification_report_result['macro avg']\n",
        "weighted_avg = classification_report_result['weighted avg']\n",
        "samples_avg = classification_report_result['samples avg']\n",
        "\n",
        "# Display the averages\n",
        "print(\"\\nMicro Average:\")\n",
        "print(f\"Precision: {micro_avg['precision']:.2f}, Recall: {micro_avg['recall']:.2f}, F1: {micro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nMacro Average:\")\n",
        "print(f\"Precision: {macro_avg['precision']:.2f}, Recall: {macro_avg['recall']:.2f}, F1: {macro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nWeighted Average:\")\n",
        "print(f\"Precision: {weighted_avg['precision']:.2f}, Recall: {weighted_avg['recall']:.2f}, F1: {weighted_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nSamples Average:\")\n",
        "print(f\"Precision: {samples_avg['precision']:.2f}, Recall: {samples_avg['recall']:.2f}, F1: {samples_avg['f1-score']:.2f}\")\n",
        "\n",
        "# Stop the CodeCarbon tracker and get the emissions\n",
        "emissions = tracker.stop()\n",
        "if emissions is None:\n",
        "    emissions = 0.0  # Default value if tracker fails\n",
        "\n",
        "print(f\"\\nTotal Carbon Emissions: {emissions:.4f} kg CO2\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98aE9Jz3ezVd",
        "outputId": "15d4e6ae-78b1-4713-a010-7b84d6f9b895"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[codecarbon WARNING @ 22:01:33] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Gradient Boosting for attribute: dark_pigmentation\n",
            "Training Gradient Boosting for attribute: acne\n",
            "Training Gradient Boosting for attribute: eye_contour\n",
            "Training Gradient Boosting for attribute: homogeneity\n",
            "Training Gradient Boosting for attribute: lack_firmness\n",
            "Training Gradient Boosting for attribute: lack_radiance\n",
            "Training Gradient Boosting for attribute: pores\n",
            "Training Gradient Boosting for attribute: fine_lines\n",
            "Training Gradient Boosting for attribute: wrinkles_fine-lines\n",
            "Training Gradient Boosting for attribute: eye-wrinkles\n",
            "Training Gradient Boosting for attribute: undereye-bags\n",
            "Training Gradient Boosting for attribute: generic\n",
            "Training Gradient Boosting for attribute: 18-34\n",
            "Training Gradient Boosting for attribute: 35-54\n",
            "Training Gradient Boosting for attribute: 55-99\n",
            "Training Gradient Boosting for attribute: dry\n",
            "Training Gradient Boosting for attribute: normal\n",
            "Training Gradient Boosting for attribute: oily\n",
            "Training Gradient Boosting for attribute: combination\n",
            "Training Gradient Boosting for attribute: sensitivity-high\n",
            "Training Gradient Boosting for attribute: sensitivity-low\n",
            "Training Gradient Boosting for attribute: no_sensitivity\n",
            "Training Gradient Boosting for attribute: male\n",
            "Training Gradient Boosting for attribute: female\n",
            "Training Gradient Boosting for attribute: cleanse\n",
            "Training Gradient Boosting for attribute: prepare\n",
            "Training Gradient Boosting for attribute: treat\n",
            "Training Gradient Boosting for attribute: targeted\n",
            "Training Gradient Boosting for attribute: care\n",
            "Training Gradient Boosting for attribute: moisturize\n",
            "Training Gradient Boosting for attribute: protect\n",
            "Training Gradient Boosting for attribute: day\n",
            "Training Gradient Boosting for attribute: night\n",
            "Gradient Boosting Results:\n",
            "                   Attribute  Accuracy        F1    Recall  Precision\n",
            "0          dark_pigmentation  0.963141  0.859756  0.870370   0.849398\n",
            "1                       acne  0.987179  0.937500  0.923077   0.952381\n",
            "2                eye_contour  0.983173  0.930233  0.965517   0.897436\n",
            "3                homogeneity  0.938301  0.763077  0.789809   0.738095\n",
            "4              lack_firmness  0.945513  0.840376  0.802691   0.881773\n",
            "5              lack_radiance  0.906250  0.858182  0.829040   0.889447\n",
            "6                      pores  0.936699  0.849524  0.813869   0.888446\n",
            "7                 fine_lines  0.910256  0.848238  0.812987   0.886686\n",
            "8        wrinkles_fine-lines  0.911058  0.869565  0.824053   0.920398\n",
            "9               eye-wrinkles  0.978365  0.895753  0.935484   0.859259\n",
            "10             undereye-bags  0.976763  0.877637  0.928571   0.832000\n",
            "11                   generic  0.767628  0.814103  0.862772   0.770631\n",
            "12                     18-34  0.794071  0.826703  0.853760   0.801307\n",
            "13                     35-54  0.846154  0.811024  0.771536   0.854772\n",
            "14                     55-99  0.873397  0.744337  0.723270   0.766667\n",
            "15                       dry  0.873397  0.791005  0.774611   0.808108\n",
            "16                    normal  0.811699  0.718563  0.674157   0.769231\n",
            "17                      oily  0.926282  0.827715  0.797834   0.859922\n",
            "18               combination  0.808494  0.703106  0.672209   0.736979\n",
            "19          sensitivity-high  0.920673  0.762590  0.807107   0.722727\n",
            "20           sensitivity-low  0.859776  0.285714  0.265152   0.309735\n",
            "21            no_sensitivity  0.769231  0.838020  0.884798   0.795940\n",
            "22                      male  0.977564  0.899281  0.886525   0.912409\n",
            "23                    female  0.811699  0.861193  0.851636   0.870968\n",
            "24                   cleanse  0.943910  0.859438  0.902954   0.819923\n",
            "25                   prepare  0.907853  0.747253  0.729614   0.765766\n",
            "26                     treat  0.861378  0.846222  0.824957   0.868613\n",
            "27                  targeted  0.890224  0.835928  0.823113   0.849148\n",
            "28                      care  0.757212  0.745592  0.758974   0.732673\n",
            "29                moisturize  0.886218  0.910127  0.893168   0.927742\n",
            "30                   protect  0.896635  0.825911  0.770781   0.889535\n",
            "31                       day  0.799679  0.869247  0.866528   0.871983\n",
            "32                     night  0.806891  0.825993  0.820660   0.831395\n",
            "Average              Average  0.885659  0.814512  0.809442   0.822166\n",
            "\n",
            "Micro Average:\n",
            "Precision: 0.83, Recall: 0.82, F1: 0.82\n",
            "\n",
            "Macro Average:\n",
            "Precision: 0.82, Recall: 0.81, F1: 0.81\n",
            "\n",
            "Weighted Average:\n",
            "Precision: 0.83, Recall: 0.82, F1: 0.82\n",
            "\n",
            "Samples Average:\n",
            "Precision: 0.82, Recall: 0.82, F1: 0.80\n",
            "\n",
            "Total Carbon Emissions: 0.0128 kg CO2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regression + LightGBM 3**"
      ],
      "metadata": {
        "id": "p5GFTAw5gqN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import lightgbm as lgb\n",
        "from codecarbon import EmissionsTracker\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Suppress scikit-learn deprecation warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Disable CodeCarbon logs\n",
        "logging.getLogger(\"codecarbon\").setLevel(logging.WARNING)\n",
        "\n",
        "# Delete the lock file if it exists\n",
        "lock_file = \"/tmp/.codecarbon.lock\"\n",
        "if os.path.exists(lock_file):\n",
        "    os.remove(lock_file)\n",
        "\n",
        "# Initialize CodeCarbon tracker (allow multiple runs)\n",
        "tracker = EmissionsTracker(log_level=\"error\", allow_multiple_runs=True)  # Suppress all logs except errors\n",
        "tracker.start()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/anthropic.claude-3-5-sonnet Full.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Enhanced Text Cleaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # Remove stopwords and lemmatize\n",
        "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df[\"text_raw\"] = df[\"text_raw\"].apply(clean_text)\n",
        "\n",
        "# Features: 'text_raw' column\n",
        "X = df[\"text_raw\"]\n",
        "\n",
        "# Labels: Binary attributes (columns 1 to 33)\n",
        "binary_columns = df.columns[1:34]  # Assuming columns 1 to 33 are binary attributes\n",
        "y = df[binary_columns]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data using TF-IDF with character n-grams\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))  # Use character n-grams\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression + LightGBM ensemble for each binary attribute\n",
        "results_ensemble = []\n",
        "y_pred_all = []  # To store predictions for all attributes\n",
        "\n",
        "for i, col in enumerate(binary_columns):\n",
        "    print(f\"Training Ensemble (Logistic Regression + LightGBM) for attribute: {col}\")\n",
        "\n",
        "    # Calculate class weights for imbalanced data\n",
        "    class_weights = {0: 1, 1: len(y_train[col]) / sum(y_train[col])}  # Higher weight for minority class\n",
        "\n",
        "    # Define Logistic Regression model\n",
        "    lr_model = LogisticRegression(\n",
        "        class_weight=\"balanced\",  # Handle imbalanced classes\n",
        "        max_iter=1000,           # Increase iterations for convergence\n",
        "        n_jobs=-1,               # Use all cores\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Define LightGBM model\n",
        "    lgb_model = lgb.LGBMClassifier(\n",
        "        n_estimators=150,  # Reduced number of trees\n",
        "        learning_rate=0.1,  # Slightly higher learning rate for faster convergence\n",
        "        max_depth=5,        # Reduced depth to prevent overfitting\n",
        "        random_state=42,\n",
        "        n_jobs=-1,          # Use all available CPU cores\n",
        "        class_weight=class_weights,  # Handle class imbalance\n",
        "        verbosity=-1,  # Suppress LightGBM warnings\n",
        "        subsample=0.8,  # Subsample to reduce computation\n",
        "        colsample_bytree=0.8  # Feature subsampling to reduce computation\n",
        "    )\n",
        "\n",
        "    # Create Voting Classifier (ensemble of Logistic Regression and LightGBM)\n",
        "    ensemble_model = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('lr', lr_model),\n",
        "            ('lgb', lgb_model)\n",
        "        ],\n",
        "        voting='soft',  # Use soft voting for probabilistic predictions\n",
        "        n_jobs=-1       # Use all cores\n",
        "    )\n",
        "\n",
        "    # Train the ensemble model\n",
        "    ensemble_model.fit(X_train_tfidf, y_train[col])\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = ensemble_model.predict(X_test_tfidf)\n",
        "    y_pred_all.append(y_pred)  # Store predictions\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test[col], y_pred)\n",
        "    f1 = f1_score(y_test[col], y_pred, average='binary')\n",
        "    recall = recall_score(y_test[col], y_pred, average='binary')\n",
        "    precision = precision_score(y_test[col], y_pred, average='binary')\n",
        "\n",
        "    results_ensemble.append([col, accuracy, f1, recall, precision])\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_ensemble_df = pd.DataFrame(results_ensemble, columns=[\"Attribute\", \"Accuracy\", \"F1\", \"Recall\", \"Precision\"])\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_accuracy_ensemble = results_ensemble_df[\"Accuracy\"].mean()\n",
        "avg_f1_ensemble = results_ensemble_df[\"F1\"].mean()\n",
        "avg_recall_ensemble = results_ensemble_df[\"Recall\"].mean()\n",
        "avg_precision_ensemble = results_ensemble_df[\"Precision\"].mean()\n",
        "\n",
        "# Add average row to the results table\n",
        "results_ensemble_df.loc[\"Average\"] = [\"Average\", avg_accuracy_ensemble, avg_f1_ensemble, avg_recall_ensemble, avg_precision_ensemble]\n",
        "\n",
        "# Display results\n",
        "print(\"Ensemble (Logistic Regression + LightGBM) Results:\")\n",
        "print(results_ensemble_df)\n",
        "\n",
        "# Convert predictions to a 2D array (num_samples, num_attributes)\n",
        "y_pred_all = np.array(y_pred_all).T\n",
        "\n",
        "# Generate classification report for micro, macro, weighted, and samples averages\n",
        "classification_report_result = classification_report(\n",
        "    y_test, y_pred_all, target_names=binary_columns, output_dict=True\n",
        ")\n",
        "\n",
        "# Extract micro, macro, weighted, and samples averages\n",
        "micro_avg = classification_report_result['micro avg']\n",
        "macro_avg = classification_report_result['macro avg']\n",
        "weighted_avg = classification_report_result['weighted avg']\n",
        "samples_avg = classification_report_result['samples avg']\n",
        "\n",
        "# Display the averages\n",
        "print(\"\\nMicro Average:\")\n",
        "print(f\"Precision: {micro_avg['precision']:.2f}, Recall: {micro_avg['recall']:.2f}, F1: {micro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nMacro Average:\")\n",
        "print(f\"Precision: {macro_avg['precision']:.2f}, Recall: {macro_avg['recall']:.2f}, F1: {macro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nWeighted Average:\")\n",
        "print(f\"Precision: {weighted_avg['precision']:.2f}, Recall: {weighted_avg['recall']:.2f}, F1: {weighted_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nSamples Average:\")\n",
        "print(f\"Precision: {samples_avg['precision']:.2f}, Recall: {samples_avg['recall']:.2f}, F1: {samples_avg['f1-score']:.2f}\")\n",
        "\n",
        "# Stop the CodeCarbon tracker and get the emissions\n",
        "emissions = tracker.stop()\n",
        "if emissions is None:\n",
        "    emissions = 0.0  # Default value if tracker fails\n",
        "\n",
        "print(f\"\\nTotal Carbon Emissions: {emissions:.4f} kg CO2\")\n"
      ],
      "metadata": {
        "id": "3PyzGgjyezY3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afc3b78c-588f-4d6a-a805-e4c628c34afb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[codecarbon WARNING @ 21:05:52] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: dark_pigmentation\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: acne\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: eye_contour\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: homogeneity\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: lack_firmness\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: lack_radiance\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: pores\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: fine_lines\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: wrinkles_fine-lines\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: eye-wrinkles\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: undereye-bags\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: generic\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: 18-34\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: 35-54\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: 55-99\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: dry\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: normal\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: oily\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: combination\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: sensitivity-high\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: sensitivity-low\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: no_sensitivity\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: male\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: female\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: cleanse\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: prepare\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: treat\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: targeted\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: care\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: moisturize\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: protect\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: day\n",
            "Training Ensemble (Logistic Regression + LightGBM) for attribute: night\n",
            "Ensemble (Logistic Regression + LightGBM) Results:\n",
            "                   Attribute  Accuracy        F1    Recall  Precision\n",
            "0          dark_pigmentation  0.961538  0.857988  0.895062   0.823864\n",
            "1                       acne  0.985577  0.929688  0.915385   0.944444\n",
            "2                eye_contour  0.986378  0.941581  0.944828   0.938356\n",
            "3                homogeneity  0.935897  0.768786  0.847134   0.703704\n",
            "4              lack_firmness  0.934295  0.827004  0.878924   0.780876\n",
            "5              lack_radiance  0.911058  0.873432  0.896956   0.851111\n",
            "6                      pores  0.939904  0.865832  0.883212   0.849123\n",
            "7                 fine_lines  0.918269  0.869898  0.885714   0.854637\n",
            "8        wrinkles_fine-lines  0.927885  0.900222  0.904232   0.896247\n",
            "9               eye-wrinkles  0.976763  0.885375  0.903226   0.868217\n",
            "10             undereye-bags  0.982372  0.905172  0.937500   0.875000\n",
            "11                   generic  0.784455  0.836474  0.934783   0.756876\n",
            "12                     18-34  0.826122  0.858999  0.920613   0.805116\n",
            "13                     35-54  0.844551  0.824910  0.855805   0.796167\n",
            "14                     55-99  0.885417  0.787519  0.833333   0.746479\n",
            "15                       dry  0.891827  0.837153  0.898964   0.783296\n",
            "16                    normal  0.808494  0.756867  0.835955   0.691450\n",
            "17                      oily  0.925481  0.835398  0.851986   0.819444\n",
            "18               combination  0.789263  0.718717  0.798100   0.653696\n",
            "19          sensitivity-high  0.927083  0.784870  0.842640   0.734513\n",
            "20           sensitivity-low  0.810096  0.387597  0.568182   0.294118\n",
            "21            no_sensitivity  0.793269  0.860390  0.944181   0.790258\n",
            "22                      male  0.979968  0.911661  0.914894   0.908451\n",
            "23                    female  0.833333  0.882486  0.912383   0.854486\n",
            "24                   cleanse  0.954327  0.883910  0.915612   0.854331\n",
            "25                   prepare  0.919872  0.796748  0.841202   0.756757\n",
            "26                     treat  0.876603  0.868825  0.883882   0.854271\n",
            "27                  targeted  0.899840  0.859708  0.903302   0.820128\n",
            "28                      care  0.768429  0.779558  0.873504   0.703857\n",
            "29                moisturize  0.899840  0.923920  0.942857   0.905728\n",
            "30                   protect  0.882212  0.814628  0.813602   0.815657\n",
            "31                       day  0.834135  0.896448  0.934307   0.861538\n",
            "32                     night  0.840545  0.867245  0.932568   0.810474\n",
            "Average              Average  0.891973  0.836334  0.880146   0.800081\n",
            "\n",
            "Micro Average:\n",
            "Precision: 0.80, Recall: 0.89, F1: 0.84\n",
            "\n",
            "Macro Average:\n",
            "Precision: 0.80, Recall: 0.88, F1: 0.84\n",
            "\n",
            "Weighted Average:\n",
            "Precision: 0.81, Recall: 0.89, F1: 0.85\n",
            "\n",
            "Samples Average:\n",
            "Precision: 0.79, Recall: 0.89, F1: 0.83\n",
            "\n",
            "Total Carbon Emissions: 0.0002 kg CO2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regression + XGBoost**"
      ],
      "metadata": {
        "id": "GjeIFF2bgu4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from codecarbon import EmissionsTracker\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Suppress scikit-learn deprecation warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Disable CodeCarbon logs\n",
        "logging.getLogger(\"codecarbon\").setLevel(logging.WARNING)\n",
        "\n",
        "# Delete the lock file if it exists\n",
        "lock_file = \"/tmp/.codecarbon.lock\"\n",
        "if os.path.exists(lock_file):\n",
        "    os.remove(lock_file)\n",
        "\n",
        "# Initialize CodeCarbon tracker (allow multiple runs)\n",
        "tracker = EmissionsTracker(log_level=\"error\", allow_multiple_runs=True)  # Suppress all logs except errors\n",
        "tracker.start()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/anthropic.claude-3-5-sonnet Full.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Enhanced Text Cleaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # Remove stopwords and lemmatize\n",
        "    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df[\"text_raw\"] = df[\"text_raw\"].apply(clean_text)\n",
        "\n",
        "# Features: 'text_raw' column\n",
        "X = df[\"text_raw\"]\n",
        "\n",
        "# Labels: Binary attributes (columns 1 to 33)\n",
        "binary_columns = df.columns[1:34]  # Assuming columns 1 to 33 are binary attributes\n",
        "y = df[binary_columns]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data using TF-IDF with character n-grams\n",
        "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 3))  # Increased max_features and ngram_range\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression + XGBoost ensemble for each binary attribute\n",
        "results_ensemble = []\n",
        "y_pred_all = []  # To store predictions for all attributes\n",
        "\n",
        "for i, col in enumerate(binary_columns):\n",
        "    print(f\"Training Ensemble (Logistic Regression + XGBoost) for attribute: {col}\")\n",
        "\n",
        "    # Handle class imbalance using SMOTE\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_res, y_train_res = smote.fit_resample(X_train_tfidf, y_train[col])\n",
        "\n",
        "    # Define Logistic Regression model with hyperparameter tuning\n",
        "    lr_model = LogisticRegression(\n",
        "        class_weight=\"balanced\",  # Handle imbalanced classes\n",
        "        max_iter=1000,           # Increase iterations for convergence\n",
        "        n_jobs=-1,               # Use all cores\n",
        "        random_state=42,\n",
        "        C=0.1,                   # Regularization parameter\n",
        "        solver='liblinear'       # Solver for better performance on small datasets\n",
        "    )\n",
        "\n",
        "    # Define XGBoost model with hyperparameter tuning\n",
        "    xgb_model = XGBClassifier(\n",
        "        n_estimators=200,  # Increased number of trees\n",
        "        learning_rate=0.05,  # Lower learning rate for better generalization\n",
        "        max_depth=7,        # Increased depth for more complex models\n",
        "        random_state=42,\n",
        "        n_jobs=-1,          # Use all available CPU cores\n",
        "        scale_pos_weight=(len(y_train_res) - sum(y_train_res)) / sum(y_train_res),  # Handle class imbalance\n",
        "        subsample=0.9,  # Subsample to reduce computation\n",
        "        colsample_bytree=0.9  # Feature subsampling to reduce computation\n",
        "    )\n",
        "\n",
        "    # Create Voting Classifier (ensemble of Logistic Regression and XGBoost)\n",
        "    ensemble_model = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('lr', lr_model),\n",
        "            ('xgb', xgb_model)\n",
        "        ],\n",
        "        voting='soft',  # Use soft voting for probabilistic predictions\n",
        "        n_jobs=-1       # Use all cores\n",
        "    )\n",
        "\n",
        "    # Train the ensemble model\n",
        "    ensemble_model.fit(X_train_res, y_train_res)\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = ensemble_model.predict(X_test_tfidf)\n",
        "    y_pred_all.append(y_pred)  # Store predictions\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test[col], y_pred)\n",
        "    f1 = f1_score(y_test[col], y_pred, average='binary')\n",
        "    recall = recall_score(y_test[col], y_pred, average='binary')\n",
        "    precision = precision_score(y_test[col], y_pred, average='binary')\n",
        "\n",
        "    results_ensemble.append([col, accuracy, f1, recall, precision])\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_ensemble_df = pd.DataFrame(results_ensemble, columns=[\"Attribute\", \"Accuracy\", \"F1\", \"Recall\", \"Precision\"])\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_accuracy_ensemble = results_ensemble_df[\"Accuracy\"].mean()\n",
        "avg_f1_ensemble = results_ensemble_df[\"F1\"].mean()\n",
        "avg_recall_ensemble = results_ensemble_df[\"Recall\"].mean()\n",
        "avg_precision_ensemble = results_ensemble_df[\"Precision\"].mean()\n",
        "\n",
        "# Add average row to the results table\n",
        "results_ensemble_df.loc[\"Average\"] = [\"Average\", avg_accuracy_ensemble, avg_f1_ensemble, avg_recall_ensemble, avg_precision_ensemble]\n",
        "\n",
        "# Display results\n",
        "print(\"Ensemble (Logistic Regression + XGBoost) Results:\")\n",
        "print(results_ensemble_df)\n",
        "\n",
        "# Convert predictions to a 2D array (num_samples, num_attributes)\n",
        "y_pred_all = np.array(y_pred_all).T\n",
        "\n",
        "# Generate classification report for micro, macro, weighted, and samples averages\n",
        "classification_report_result = classification_report(\n",
        "    y_test, y_pred_all, target_names=binary_columns, output_dict=True\n",
        ")\n",
        "\n",
        "# Extract micro, macro, weighted, and samples averages\n",
        "micro_avg = classification_report_result['micro avg']\n",
        "macro_avg = classification_report_result['macro avg']\n",
        "weighted_avg = classification_report_result['weighted avg']\n",
        "samples_avg = classification_report_result['samples avg']\n",
        "\n",
        "# Display the averages\n",
        "print(\"\\nMicro Average:\")\n",
        "print(f\"Precision: {micro_avg['precision']:.2f}, Recall: {micro_avg['recall']:.2f}, F1: {micro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nMacro Average:\")\n",
        "print(f\"Precision: {macro_avg['precision']:.2f}, Recall: {macro_avg['recall']:.2f}, F1: {macro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nWeighted Average:\")\n",
        "print(f\"Precision: {weighted_avg['precision']:.2f}, Recall: {weighted_avg['recall']:.2f}, F1: {weighted_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nSamples Average:\")\n",
        "print(f\"Precision: {samples_avg['precision']:.2f}, Recall: {samples_avg['recall']:.2f}, F1: {samples_avg['f1-score']:.2f}\")\n",
        "\n",
        "# Stop the CodeCarbon tracker and get the emissions\n",
        "emissions = tracker.stop()\n",
        "if emissions is None:\n",
        "    emissions = 0.0  # Default value if tracker fails\n",
        "\n",
        "print(f\"\\nTotal Carbon Emissions: {emissions:.4f} kg CO2\")"
      ],
      "metadata": {
        "id": "WgInSGlaezgO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac34e58-e613-44b3-fe9f-2b5299102bb2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[codecarbon WARNING @ 20:25:57] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: dark_pigmentation\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: acne\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: eye_contour\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: homogeneity\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: lack_firmness\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: lack_radiance\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: pores\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: fine_lines\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: wrinkles_fine-lines\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: eye-wrinkles\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: undereye-bags\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: generic\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: 18-34\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: 35-54\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: 55-99\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: dry\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: normal\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: oily\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: combination\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: sensitivity-high\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: sensitivity-low\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: no_sensitivity\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: male\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: female\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: cleanse\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: prepare\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: treat\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: targeted\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: care\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: moisturize\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: protect\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: day\n",
            "Training Ensemble (Logistic Regression + XGBoost) for attribute: night\n",
            "Ensemble (Logistic Regression + XGBoost) Results:\n",
            "                   Attribute  Accuracy        F1    Recall  Precision\n",
            "0          dark_pigmentation  0.965545  0.868502  0.876543   0.860606\n",
            "1                       acne  0.987179  0.936508  0.907692   0.967213\n",
            "2                eye_contour  0.987981  0.946996  0.924138   0.971014\n",
            "3                homogeneity  0.947115  0.792453  0.802548   0.782609\n",
            "4              lack_firmness  0.953526  0.865741  0.838565   0.894737\n",
            "5              lack_radiance  0.908654  0.863309  0.843091   0.884521\n",
            "6                      pores  0.943109  0.865275  0.832117   0.901186\n",
            "7                 fine_lines  0.923077  0.870968  0.841558   0.902507\n",
            "8        wrinkles_fine-lines  0.926282  0.893271  0.857461   0.932203\n",
            "9               eye-wrinkles  0.981571  0.905350  0.887097   0.924370\n",
            "10             undereye-bags  0.978365  0.880000  0.883929   0.876106\n",
            "11                   generic  0.775641  0.820051  0.866848   0.778049\n",
            "12                     18-34  0.814103  0.842818  0.866295   0.820580\n",
            "13                     35-54  0.849359  0.819231  0.797753   0.841897\n",
            "14                     55-99  0.885417  0.768233  0.745283   0.792642\n",
            "15                       dry  0.890224  0.820446  0.810881   0.830239\n",
            "16                    normal  0.829327  0.751459  0.723596   0.781553\n",
            "17                      oily  0.924679  0.825926  0.805054   0.847909\n",
            "18               combination  0.816506  0.718327  0.693587   0.744898\n",
            "19          sensitivity-high  0.925481  0.771499  0.796954   0.747619\n",
            "20           sensitivity-low  0.872596  0.311688  0.272727   0.363636\n",
            "21            no_sensitivity  0.784455  0.848791  0.896675   0.805763\n",
            "22                      male  0.979167  0.905109  0.879433   0.932331\n",
            "23                    female  0.815705  0.863744  0.851636   0.876202\n",
            "24                   cleanse  0.954327  0.881988  0.898734   0.865854\n",
            "25                   prepare  0.904647  0.738462  0.721030   0.756757\n",
            "26                     treat  0.873397  0.861160  0.849220   0.873440\n",
            "27                  targeted  0.894231  0.842857  0.834906   0.850962\n",
            "28                      care  0.764423  0.754591  0.772650   0.737357\n",
            "29                moisturize  0.897436  0.920596  0.921739   0.919455\n",
            "30                   protect  0.903045  0.839309  0.795970   0.887640\n",
            "31                       day  0.819712  0.883238  0.887383   0.879132\n",
            "32                     night  0.828526  0.849296  0.865136   0.834025\n",
            "Average              Average  0.894085  0.828097  0.819643   0.838334\n",
            "\n",
            "Micro Average:\n",
            "Precision: 0.84, Recall: 0.83, F1: 0.84\n",
            "\n",
            "Macro Average:\n",
            "Precision: 0.84, Recall: 0.82, F1: 0.83\n",
            "\n",
            "Weighted Average:\n",
            "Precision: 0.84, Recall: 0.83, F1: 0.84\n",
            "\n",
            "Samples Average:\n",
            "Precision: 0.83, Recall: 0.83, F1: 0.82\n",
            "\n",
            "Total Carbon Emissions: 0.0071 kg CO2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Random Forest **"
      ],
      "metadata": {
        "id": "9RY4rE6EhH3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from codecarbon import EmissionsTracker\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "logging.getLogger(\"codecarbon\").setLevel(logging.WARNING)\n",
        "\n",
        "# Delete the lock file if it exists\n",
        "lock_file = \"/tmp/.codecarbon.lock\"\n",
        "if os.path.exists(lock_file):\n",
        "    os.remove(lock_file)\n",
        "\n",
        "# Initialize CodeCarbon tracker\n",
        "tracker = EmissionsTracker(log_level=\"error\", allow_multiple_runs=True)\n",
        "tracker.start()\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/anthropic.claude-3-5-sonnet Full.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df[\"text_raw\"] = df[\"text_raw\"].apply(clean_text)\n",
        "\n",
        "# Features and labels\n",
        "X = df[\"text_raw\"]\n",
        "binary_columns = df.columns[1:34]\n",
        "y = df[binary_columns]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Random Forest for each attribute\n",
        "results_rf = []\n",
        "y_pred_all = []\n",
        "\n",
        "for col in binary_columns:\n",
        "    print(f\"Training Random Forest for attribute: {col}\")\n",
        "\n",
        "    class_weights = {0: 1, 1: len(y_train[col]) / sum(y_train[col])}\n",
        "\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=150,  # Reduce number of trees for efficiency\n",
        "        max_depth=10,  # Moderate depth to balance accuracy and emissions\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        class_weight=class_weights,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train_tfidf, y_train[col])\n",
        "\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "    y_pred_all.append(y_pred)\n",
        "\n",
        "    accuracy = accuracy_score(y_test[col], y_pred)\n",
        "    f1 = f1_score(y_test[col], y_pred, average='binary')\n",
        "    recall = recall_score(y_test[col], y_pred, average='binary')\n",
        "    precision = precision_score(y_test[col], y_pred, average='binary')\n",
        "\n",
        "    results_rf.append([col, accuracy, f1, recall, precision])\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_rf_df = pd.DataFrame(results_rf, columns=[\"Attribute\", \"Accuracy\", \"F1\", \"Recall\", \"Precision\"])\n",
        "\n",
        "# Compute averages\n",
        "avg_metrics = results_rf_df.mean(numeric_only=True)\n",
        "results_rf_df.loc[\"Average\"] = [\"Average\"] + avg_metrics.tolist()\n",
        "\n",
        "print(\"Random Forest Results:\")\n",
        "print(results_rf_df)\n",
        "\n",
        "# Convert predictions to 2D array\n",
        "y_pred_all = np.array(y_pred_all).T\n",
        "\n",
        "# Generate classification report\n",
        "classification_report_result = classification_report(y_test, y_pred_all, target_names=binary_columns, output_dict=True)\n",
        "\n",
        "print(\"\\nClassification Report Averages:\")\n",
        "for avg_type in [\"micro avg\", \"macro avg\", \"weighted avg\", \"samples avg\"]:\n",
        "    avg = classification_report_result.get(avg_type, {})\n",
        "    print(f\"{avg_type.capitalize()} - Precision: {avg.get('precision', 0):.2f}, Recall: {avg.get('recall', 0):.2f}, F1: {avg.get('f1-score', 0):.2f}\")\n",
        "\n",
        "# Stop emissions tracker\n",
        "emissions = tracker.stop() or 0.0\n",
        "print(f\"\\nTotal Carbon Emissions: {emissions:.4f} kg CO2\")\n"
      ],
      "metadata": {
        "id": "bPC1uC3KezjV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6c0c20b-e5a9-4b3d-9960-0ab2e92f23b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon WARNING @ 20:25:22] Multiple instances of codecarbon are allowed to run at the same time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Random Forest for attribute: dark_pigmentation\n",
            "Training Random Forest for attribute: acne\n",
            "Training Random Forest for attribute: eye_contour\n",
            "Training Random Forest for attribute: homogeneity\n",
            "Training Random Forest for attribute: lack_firmness\n",
            "Training Random Forest for attribute: lack_radiance\n",
            "Training Random Forest for attribute: pores\n",
            "Training Random Forest for attribute: fine_lines\n",
            "Training Random Forest for attribute: wrinkles_fine-lines\n",
            "Training Random Forest for attribute: eye-wrinkles\n",
            "Training Random Forest for attribute: undereye-bags\n",
            "Training Random Forest for attribute: generic\n",
            "Training Random Forest for attribute: 18-34\n",
            "Training Random Forest for attribute: 35-54\n",
            "Training Random Forest for attribute: 55-99\n",
            "Training Random Forest for attribute: dry\n",
            "Training Random Forest for attribute: normal\n",
            "Training Random Forest for attribute: oily\n",
            "Training Random Forest for attribute: combination\n",
            "Training Random Forest for attribute: sensitivity-high\n",
            "Training Random Forest for attribute: sensitivity-low\n",
            "Training Random Forest for attribute: no_sensitivity\n",
            "Training Random Forest for attribute: male\n",
            "Training Random Forest for attribute: female\n",
            "Training Random Forest for attribute: cleanse\n",
            "Training Random Forest for attribute: prepare\n",
            "Training Random Forest for attribute: treat\n",
            "Training Random Forest for attribute: targeted\n",
            "Training Random Forest for attribute: care\n",
            "Training Random Forest for attribute: moisturize\n",
            "Training Random Forest for attribute: protect\n",
            "Training Random Forest for attribute: day\n",
            "Training Random Forest for attribute: night\n",
            "Random Forest Results:\n",
            "                   Attribute  Accuracy        F1    Recall  Precision\n",
            "0          dark_pigmentation  0.943109  0.790560  0.827160   0.757062\n",
            "1                       acne  0.971154  0.857143  0.830769   0.885246\n",
            "2                eye_contour  0.976763  0.896797  0.868966   0.926471\n",
            "3                homogeneity  0.908654  0.693548  0.821656   0.600000\n",
            "4              lack_firmness  0.907853  0.769539  0.860987   0.695652\n",
            "5              lack_radiance  0.697115  0.683417  0.955504   0.531943\n",
            "6                      pores  0.877404  0.762053  0.894161   0.663957\n",
            "7                 fine_lines  0.858974  0.800000  0.914286   0.711111\n",
            "8        wrinkles_fine-lines  0.838942  0.809117  0.948775   0.705298\n",
            "9               eye-wrinkles  0.971955  0.853556  0.822581   0.886957\n",
            "10             undereye-bags  0.977564  0.875000  0.875000   0.875000\n",
            "11                   generic  0.624199  0.758372  1.000000   0.610788\n",
            "12                     18-34  0.675481  0.779532  0.997214   0.639857\n",
            "13                     35-54  0.602564  0.677503  0.975655   0.518924\n",
            "14                     55-99  0.830128  0.725389  0.880503   0.616740\n",
            "15                       dry  0.749199  0.702754  0.958549   0.554723\n",
            "16                    normal  0.564904  0.614072  0.970787   0.449064\n",
            "17                      oily  0.846955  0.720351  0.888087   0.605911\n",
            "18               combination  0.591346  0.611280  0.952494   0.450056\n",
            "19          sensitivity-high  0.891026  0.692308  0.776650   0.624490\n",
            "20           sensitivity-low  0.782853  0.313924  0.469697   0.235741\n",
            "21            no_sensitivity  0.680288  0.808449  1.000000   0.678485\n",
            "22                      male  0.951122  0.794613  0.836879   0.756410\n",
            "23                    female  0.697917  0.819531  1.000000   0.694242\n",
            "24                   cleanse  0.943910  0.861111  0.915612   0.812734\n",
            "25                   prepare  0.789263  0.613803  0.896996   0.466518\n",
            "26                     treat  0.626603  0.710199  0.989601   0.553831\n",
            "27                  targeted  0.748397  0.721631  0.959906   0.578125\n",
            "28                      care  0.569712  0.684674  0.996581   0.521467\n",
            "29                moisturize  0.684295  0.803197  0.998758   0.671679\n",
            "30                   protect  0.863782  0.798578  0.848866   0.753915\n",
            "31                       day  0.770032  0.869841  1.000000   0.769663\n",
            "32                     night  0.664263  0.768380  0.997131   0.625000\n",
            "Average              Average  0.790234  0.746673  0.906964   0.649305\n",
            "\n",
            "Classification Report Averages:\n",
            "Micro avg - Precision: 0.62, Recall: 0.95, F1: 0.75\n",
            "Macro avg - Precision: 0.65, Recall: 0.91, F1: 0.75\n",
            "Weighted avg - Precision: 0.64, Recall: 0.95, F1: 0.76\n",
            "Samples avg - Precision: 0.61, Recall: 0.95, F1: 0.72\n",
            "\n",
            "Total Carbon Emissions: 0.0001 kg CO2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RoBERTa + LightGBM**"
      ],
      "metadata": {
        "id": "GTibXJuBhk8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "import lightgbm as lgb\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from codecarbon import EmissionsTracker\n",
        "import logging\n",
        "\n",
        "# Disable CodeCarbon logs\n",
        "logging.getLogger(\"codecarbon\").setLevel(logging.WARNING)\n",
        "\n",
        "# Initialize CodeCarbon tracker (silent mode)\n",
        "tracker = EmissionsTracker(log_level=\"error\")  # Suppress all logs except errors\n",
        "tracker.start()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/anthropic.claude-3-5-sonnet Full.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Clean the 'text_raw' column\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df[\"text_raw\"] = df[\"text_raw\"].apply(clean_text)\n",
        "\n",
        "# Features: 'text_raw' column\n",
        "X = df[\"text_raw\"]\n",
        "\n",
        "# Labels: Binary attributes (columns 1 to 33)\n",
        "binary_columns = df.columns[1:34]  # Assuming columns 1 to 33 are binary attributes\n",
        "y = df[binary_columns]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the data using RoBERTa\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "def tokenize_data(texts, labels):\n",
        "    encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=128)\n",
        "    encodings[\"labels\"] = labels.values.tolist()  # Add labels to the encodings\n",
        "    return encodings\n",
        "\n",
        "train_encodings = tokenize_data(X_train, y_train)\n",
        "test_encodings = tokenize_data(X_test, y_test)\n",
        "\n",
        "# Convert text to TF-IDF features for LightGBM\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Create PyTorch Dataset\n",
        "class SkinConditionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "train_dataset = SkinConditionDataset(train_encodings)\n",
        "test_dataset = SkinConditionDataset(test_encodings)\n",
        "\n",
        "# Custom Trainer for Multi-Label Classification\n",
        "class MultiLabelTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss\n",
        "        loss = loss_fct(logits, labels.float())\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Load pre-trained RoBERTa model\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(binary_columns))\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = MultiLabelTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Train the RoBERTa model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate RoBERTa model\n",
        "roberta_predictions = trainer.predict(test_dataset)\n",
        "roberta_preds = (torch.sigmoid(torch.tensor(roberta_predictions.predictions)) > 0.5).int()\n",
        "\n",
        "# Train LightGBM model\n",
        "lgb_model = MultiOutputClassifier(lgb.LGBMClassifier())\n",
        "lgb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Evaluate LightGBM model\n",
        "lgb_preds = lgb_model.predict(X_test_tfidf)\n",
        "\n",
        "# Combine predictions (70% RoBERTa, 30% LightGBM)\n",
        "roberta_weight = 0.7\n",
        "lgb_weight = 0.3\n",
        "combined_preds = (roberta_weight * roberta_preds.numpy() + lgb_weight * lgb_preds) > 0.5\n",
        "\n",
        "# Calculate metrics for each attribute\n",
        "results_combined = []\n",
        "for i, col in enumerate(binary_columns):\n",
        "    accuracy = accuracy_score(y_test[col], combined_preds[:, i])\n",
        "    f1 = f1_score(y_test[col], combined_preds[:, i], average='binary')\n",
        "    recall = recall_score(y_test[col], combined_preds[:, i], average='binary')\n",
        "    precision = precision_score(y_test[col], combined_preds[:, i], average='binary')\n",
        "    results_combined.append([col, accuracy, f1, recall, precision])\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_combined_df = pd.DataFrame(results_combined, columns=[\"Attribute\", \"Accuracy\", \"F1\", \"Recall\", \"Precision\"])\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_accuracy_combined = results_combined_df[\"Accuracy\"].mean()\n",
        "avg_f1_combined = results_combined_df[\"F1\"].mean()\n",
        "avg_recall_combined = results_combined_df[\"Recall\"].mean()\n",
        "avg_precision_combined = results_combined_df[\"Precision\"].mean()\n",
        "\n",
        "# Add average row to the results table\n",
        "results_combined_df.loc[\"Average\"] = [\"Average\", avg_accuracy_combined, avg_f1_combined, avg_recall_combined, avg_precision_combined]\n",
        "\n",
        "# Display results\n",
        "print(\"Combined Results (RoBERTa + LightGBM):\")\n",
        "print(results_combined_df)\n",
        "\n",
        "# Generate classification report for micro, macro, weighted, and samples averages\n",
        "classification_report_result = classification_report(\n",
        "    y_test, combined_preds, target_names=binary_columns, output_dict=True\n",
        ")\n",
        "\n",
        "# Extract micro, macro, weighted, and samples averages\n",
        "micro_avg = classification_report_result['micro avg']\n",
        "macro_avg = classification_report_result['macro avg']\n",
        "weighted_avg = classification_report_result['weighted avg']\n",
        "samples_avg = classification_report_result['samples avg']\n",
        "\n",
        "# Display the averages\n",
        "print(\"\\nMicro Average:\")\n",
        "print(f\"Precision: {micro_avg['precision']:.2f}, Recall: {micro_avg['recall']:.2f}, F1: {micro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nMacro Average:\")\n",
        "print(f\"Precision: {macro_avg['precision']:.2f}, Recall: {macro_avg['recall']:.2f}, F1: {macro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nWeighted Average:\")\n",
        "print(f\"Precision: {weighted_avg['precision']:.2f}, Recall: {weighted_avg['recall']:.2f}, F1: {weighted_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nSamples Average:\")\n",
        "print(f\"Precision: {samples_avg['precision']:.2f}, Recall: {samples_avg['recall']:.2f}, F1: {samples_avg['f1-score']:.2f}\")\n",
        "\n",
        "# Stop the CodeCarbon tracker and get the emissions\n",
        "emissions: float = tracker.stop()\n",
        "print(f\"\\nTotal Carbon Emissions: {emissions:.4f} kg CO2\")"
      ],
      "metadata": {
        "id": "-HfS_6G4ezmc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a571529c-0801-49a8-e27a-d0dff05065f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3120' max='3120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3120/3120 24:47, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.446700</td>\n",
              "      <td>0.416291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.350600</td>\n",
              "      <td>0.327864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.294100</td>\n",
              "      <td>0.293253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.247600</td>\n",
              "      <td>0.274837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.230400</td>\n",
              "      <td>0.259873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.206500</td>\n",
              "      <td>0.255068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.186300</td>\n",
              "      <td>0.248585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.173700</td>\n",
              "      <td>0.245330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.245061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.151900</td>\n",
              "      <td>0.243642</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 592, number of negative: 4400\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.094833 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.118590 -> initscore=-2.005853\n",
            "[LightGBM] [Info] Start training from score -2.005853\n",
            "[LightGBM] [Info] Number of positive: 534, number of negative: 4458\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082285 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.106971 -> initscore=-2.122060\n",
            "[LightGBM] [Info] Start training from score -2.122060\n",
            "[LightGBM] [Info] Number of positive: 597, number of negative: 4395\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089864 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119591 -> initscore=-1.996306\n",
            "[LightGBM] [Info] Start training from score -1.996306\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 703, number of negative: 4289\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.144536 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.140825 -> initscore=-1.808452\n",
            "[LightGBM] [Info] Start training from score -1.808452\n",
            "[LightGBM] [Info] Number of positive: 844, number of negative: 4148\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092213 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.169071 -> initscore=-1.592229\n",
            "[LightGBM] [Info] Start training from score -1.592229\n",
            "[LightGBM] [Info] Number of positive: 1726, number of negative: 3266\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093161 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.345753 -> initscore=-0.637759\n",
            "[LightGBM] [Info] Start training from score -0.637759\n",
            "[LightGBM] [Info] Number of positive: 1171, number of negative: 3821\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167666 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.234575 -> initscore=-1.182654\n",
            "[LightGBM] [Info] Start training from score -1.182654\n",
            "[LightGBM] [Info] Number of positive: 1589, number of negative: 3403\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089910 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.318309 -> initscore=-0.761553\n",
            "[LightGBM] [Info] Start training from score -0.761553\n",
            "[LightGBM] [Info] Number of positive: 1845, number of negative: 3147\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094505 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.369591 -> initscore=-0.533970\n",
            "[LightGBM] [Info] Start training from score -0.533970\n",
            "[LightGBM] [Info] Number of positive: 499, number of negative: 4493\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092475 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.099960 -> initscore=-2.197670\n",
            "[LightGBM] [Info] Start training from score -2.197670\n",
            "[LightGBM] [Info] Number of positive: 472, number of negative: 4520\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104365 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.094551 -> initscore=-2.259288\n",
            "[LightGBM] [Info] Start training from score -2.259288\n",
            "[LightGBM] [Info] Number of positive: 2983, number of negative: 2009\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096379 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.597556 -> initscore=0.395292\n",
            "[LightGBM] [Info] Start training from score 0.395292\n",
            "[LightGBM] [Info] Number of positive: 2969, number of negative: 2023\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098742 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.594752 -> initscore=0.383644\n",
            "[LightGBM] [Info] Start training from score 0.383644\n",
            "[LightGBM] [Info] Number of positive: 2279, number of negative: 2713\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149049 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.456530 -> initscore=-0.174318\n",
            "[LightGBM] [Info] Start training from score -0.174318\n",
            "[LightGBM] [Info] Number of positive: 1277, number of negative: 3715\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108689 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.255809 -> initscore=-1.067865\n",
            "[LightGBM] [Info] Start training from score -1.067865\n",
            "[LightGBM] [Info] Number of positive: 1462, number of negative: 3530\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096892 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.292869 -> initscore=-0.881493\n",
            "[LightGBM] [Info] Start training from score -0.881493\n",
            "[LightGBM] [Info] Number of positive: 1773, number of negative: 3219\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.156812 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.355168 -> initscore=-0.596398\n",
            "[LightGBM] [Info] Start training from score -0.596398\n",
            "[LightGBM] [Info] Number of positive: 1114, number of negative: 3878\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094633 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.223157 -> initscore=-1.247362\n",
            "[LightGBM] [Info] Start training from score -1.247362\n",
            "[LightGBM] [Info] Number of positive: 1637, number of negative: 3355\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102577 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.327925 -> initscore=-0.717586\n",
            "[LightGBM] [Info] Start training from score -0.717586\n",
            "[LightGBM] [Info] Number of positive: 756, number of negative: 4236\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.157181 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.151442 -> initscore=-1.723333\n",
            "[LightGBM] [Info] Start training from score -1.723333\n",
            "[LightGBM] [Info] Number of positive: 523, number of negative: 4469\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095145 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.104768 -> initscore=-2.145338\n",
            "[LightGBM] [Info] Start training from score -2.145338\n",
            "[LightGBM] [Info] Number of positive: 3470, number of negative: 1522\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095169 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.695112 -> initscore=0.824129\n",
            "[LightGBM] [Info] Start training from score 0.824129\n",
            "[LightGBM] [Info] Number of positive: 648, number of negative: 4344\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094067 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.129808 -> initscore=-1.902660\n",
            "[LightGBM] [Info] Start training from score -1.902660\n",
            "[LightGBM] [Info] Number of positive: 3368, number of negative: 1624\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096432 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.674679 -> initscore=0.729427\n",
            "[LightGBM] [Info] Start training from score 0.729427\n",
            "[LightGBM] [Info] Number of positive: 1034, number of negative: 3958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097795 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.207131 -> initscore=-1.342304\n",
            "[LightGBM] [Info] Start training from score -1.342304\n",
            "[LightGBM] [Info] Number of positive: 959, number of negative: 4033\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096898 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.192107 -> initscore=-1.436375\n",
            "[LightGBM] [Info] Start training from score -1.436375\n",
            "[LightGBM] [Info] Number of positive: 2451, number of negative: 2541\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098811 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.490986 -> initscore=-0.036062\n",
            "[LightGBM] [Info] Start training from score -0.036062\n",
            "[LightGBM] [Info] Number of positive: 1801, number of negative: 3191\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093842 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.360777 -> initscore=-0.571992\n",
            "[LightGBM] [Info] Start training from score -0.571992\n",
            "[LightGBM] [Info] Number of positive: 2364, number of negative: 2628\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.109598 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.473558 -> initscore=-0.105868\n",
            "[LightGBM] [Info] Start training from score -0.105868\n",
            "[LightGBM] [Info] Number of positive: 3196, number of negative: 1796\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099581 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.640224 -> initscore=0.576338\n",
            "[LightGBM] [Info] Start training from score 0.576338\n",
            "[LightGBM] [Info] Number of positive: 1593, number of negative: 3399\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.111574 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.319111 -> initscore=-0.757862\n",
            "[LightGBM] [Info] Start training from score -0.757862\n",
            "[LightGBM] [Info] Number of positive: 3845, number of negative: 1147\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097222 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.770232 -> initscore=1.209624\n",
            "[LightGBM] [Info] Start training from score 1.209624\n",
            "[LightGBM] [Info] Number of positive: 2847, number of negative: 2145\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097533 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 86181\n",
            "[LightGBM] [Info] Number of data points in the train set: 4992, number of used features: 2117\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.570312 -> initscore=0.283126\n",
            "[LightGBM] [Info] Start training from score 0.283126\n",
            "Combined Results (RoBERTa + LightGBM):\n",
            "                   Attribute  Accuracy        F1    Recall  Precision\n",
            "0          dark_pigmentation  0.962340  0.847896  0.808642   0.891156\n",
            "1                       acne  0.983173  0.918919  0.915385   0.922481\n",
            "2                eye_contour  0.982372  0.920290  0.875862   0.969466\n",
            "3                homogeneity  0.930288  0.712871  0.687898   0.739726\n",
            "4              lack_firmness  0.935096  0.813793  0.793722   0.834906\n",
            "5              lack_radiance  0.915064  0.874408  0.864169   0.884892\n",
            "6                      pores  0.943910  0.869888  0.854015   0.886364\n",
            "7                 fine_lines  0.922276  0.875481  0.885714   0.865482\n",
            "8        wrinkles_fine-lines  0.931891  0.905240  0.904232   0.906250\n",
            "9               eye-wrinkles  0.979968  0.897959  0.887097   0.909091\n",
            "10             undereye-bags  0.975160  0.863436  0.875000   0.852174\n",
            "11                   generic  0.789263  0.828887  0.865489   0.795256\n",
            "12                     18-34  0.826122  0.850654  0.860724   0.840816\n",
            "13                     35-54  0.868590  0.848148  0.857678   0.838828\n",
            "14                     55-99  0.902244  0.805112  0.792453   0.818182\n",
            "15                       dry  0.892628  0.819892  0.790155   0.851955\n",
            "16                    normal  0.827724  0.754286  0.741573   0.767442\n",
            "17                      oily  0.925481  0.822857  0.779783   0.870968\n",
            "18               combination  0.814904  0.705732  0.657957   0.760989\n",
            "19          sensitivity-high  0.927885  0.755435  0.705584   0.812865\n",
            "20           sensitivity-low  0.895032  0.367150  0.287879   0.506667\n",
            "21            no_sensitivity  0.774840  0.841690  0.887173   0.800643\n",
            "22                      male  0.981571  0.915129  0.879433   0.953846\n",
            "23                    female  0.854968  0.896630  0.917056   0.877095\n",
            "24                   cleanse  0.963942  0.906445  0.919831   0.893443\n",
            "25                   prepare  0.919071  0.765661  0.708155   0.833333\n",
            "26                     treat  0.893429  0.882819  0.868284   0.897849\n",
            "27                  targeted  0.903846  0.856459  0.844340   0.868932\n",
            "28                      care  0.793269  0.786777  0.813675   0.761600\n",
            "29                moisturize  0.903045  0.925994  0.940373   0.912048\n",
            "30                   protect  0.882212  0.808344  0.780856   0.837838\n",
            "31                       day  0.834936  0.896690  0.932221   0.863768\n",
            "32                     night  0.841346  0.860169  0.873745   0.847010\n",
            "Average              Average  0.899330  0.830338  0.819883   0.844647\n",
            "\n",
            "Micro Average:\n",
            "Precision: 0.85, Recall: 0.85, F1: 0.85\n",
            "\n",
            "Macro Average:\n",
            "Precision: 0.84, Recall: 0.82, F1: 0.83\n",
            "\n",
            "Weighted Average:\n",
            "Precision: 0.85, Recall: 0.85, F1: 0.84\n",
            "\n",
            "Samples Average:\n",
            "Precision: 0.84, Recall: 0.85, F1: 0.83\n",
            "\n",
            "Total Carbon Emissions: 0.0069 kg CO2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RoBERTa model**"
      ],
      "metadata": {
        "id": "Q2-uqEZ-h3Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "from codecarbon import EmissionsTracker\n",
        "import logging\n",
        "\n",
        "# Disable CodeCarbon logs\n",
        "logging.getLogger(\"codecarbon\").setLevel(logging.WARNING)\n",
        "\n",
        "# Initialize CodeCarbon tracker (silent mode)\n",
        "tracker = EmissionsTracker(log_level=\"error\")  # Suppress all logs except errors\n",
        "tracker.start()\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/anthropic.claude-3-5-sonnet Full.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Clean the 'text_raw' column\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df[\"text_raw\"] = df[\"text_raw\"].apply(clean_text)\n",
        "\n",
        "# Features: 'text_raw' column\n",
        "X = df[\"text_raw\"]\n",
        "\n",
        "# Labels: Binary attributes (columns 1 to 33)\n",
        "binary_columns = df.columns[1:34]  # Assuming columns 1 to 33 are binary attributes\n",
        "y = df[binary_columns]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the data using RoBERTa\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "def tokenize_data(texts, labels):\n",
        "    encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=128)\n",
        "    encodings[\"labels\"] = labels.values.tolist()  # Add labels to the encodings\n",
        "    return encodings\n",
        "\n",
        "train_encodings = tokenize_data(X_train, y_train)\n",
        "test_encodings = tokenize_data(X_test, y_test)\n",
        "\n",
        "# Create PyTorch Dataset\n",
        "class SkinConditionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "train_dataset = SkinConditionDataset(train_encodings)\n",
        "test_dataset = SkinConditionDataset(test_encodings)\n",
        "\n",
        "# Custom Trainer for Multi-Label Classification\n",
        "class MultiLabelTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss\n",
        "        loss = loss_fct(logits, labels.float())\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Load pre-trained RoBERTa model\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(binary_columns))\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = MultiLabelTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Train the RoBERTa model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate RoBERTa model\n",
        "roberta_predictions = trainer.predict(test_dataset)\n",
        "roberta_preds = (torch.sigmoid(torch.tensor(roberta_predictions.predictions)) > 0.5).int()\n",
        "\n",
        "# Calculate metrics for each attribute\n",
        "results_roberta = []\n",
        "for i, col in enumerate(binary_columns):\n",
        "    accuracy = accuracy_score(y_test[col], roberta_preds[:, i])\n",
        "    f1 = f1_score(y_test[col], roberta_preds[:, i], average='binary')\n",
        "    recall = recall_score(y_test[col], roberta_preds[:, i], average='binary')\n",
        "    precision = precision_score(y_test[col], roberta_preds[:, i], average='binary')\n",
        "    results_roberta.append([col, accuracy, f1, recall, precision])\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_roberta_df = pd.DataFrame(results_roberta, columns=[\"Attribute\", \"Accuracy\", \"F1\", \"Recall\", \"Precision\"])\n",
        "\n",
        "# Calculate average metrics\n",
        "avg_accuracy_roberta = results_roberta_df[\"Accuracy\"].mean()\n",
        "avg_f1_roberta = results_roberta_df[\"F1\"].mean()\n",
        "avg_recall_roberta = results_roberta_df[\"Recall\"].mean()\n",
        "avg_precision_roberta = results_roberta_df[\"Precision\"].mean()\n",
        "\n",
        "# Add average row to the results table\n",
        "results_roberta_df.loc[\"Average\"] = [\"Average\", avg_accuracy_roberta, avg_f1_roberta, avg_recall_roberta, avg_precision_roberta]\n",
        "\n",
        "# Display results\n",
        "print(\"RoBERTa Results:\")\n",
        "print(results_roberta_df)\n",
        "\n",
        "# Generate classification report for micro, macro, weighted, and samples averages\n",
        "classification_report_result = classification_report(\n",
        "    y_test, roberta_preds, target_names=binary_columns, output_dict=True\n",
        ")\n",
        "\n",
        "# Extract micro, macro, weighted, and samples averages\n",
        "micro_avg = classification_report_result['micro avg']\n",
        "macro_avg = classification_report_result['macro avg']\n",
        "weighted_avg = classification_report_result['weighted avg']\n",
        "samples_avg = classification_report_result['samples avg']\n",
        "\n",
        "# Display the averages\n",
        "print(\"\\nMicro Average:\")\n",
        "print(f\"Precision: {micro_avg['precision']:.2f}, Recall: {micro_avg['recall']:.2f}, F1: {micro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nMacro Average:\")\n",
        "print(f\"Precision: {macro_avg['precision']:.2f}, Recall: {macro_avg['recall']:.2f}, F1: {macro_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nWeighted Average:\")\n",
        "print(f\"Precision: {weighted_avg['precision']:.2f}, Recall: {weighted_avg['recall']:.2f}, F1: {weighted_avg['f1-score']:.2f}\")\n",
        "\n",
        "print(\"\\nSamples Average:\")\n",
        "print(f\"Precision: {samples_avg['precision']:.2f}, Recall: {samples_avg['recall']:.2f}, F1: {samples_avg['f1-score']:.2f}\")\n",
        "\n",
        "# Stop the CodeCarbon tracker and get the emissions\n",
        "emissions: float = tracker.stop()\n",
        "print(f\"\\nTotal Carbon Emissions: {emissions:.4f} kg CO2\")\n"
      ],
      "metadata": {
        "id": "bL-BiZbjezpi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a33103c97b4d41e7b2425cc95a147ec1",
            "516a0b5beed144f0bf0879dd23e2f4f2",
            "e4ce911b01f647e2b95b7c04a2e99df6",
            "995ece097c6745539d40f2dadfcea0c0",
            "4bd0e132bf894867882f2e5e10c79351",
            "1e9953d6aa39468ca518bd179cbaf6a3",
            "a8849e2d8d6a4e16b91bfe4f7f3d4f24",
            "c09858832b7f4c6099e89a1b72745af8",
            "05b2dd364e34475899a6b90d0668a260",
            "2e19deda79dd4b08958b8f906b66ea7c",
            "68a80e4518e245bebfa91c96f11a57a8",
            "e151a284d02642b7b32dd25a347044e6",
            "bee504562e68414394de2f70848c4b63",
            "95475555304d45cb91cf9dd3bb681c73",
            "1420a802ca00484aa33dc95c232c1db5",
            "f6d18f69ce97455fb820d20fa6067d2b",
            "678c1aef7aab48be9128c64259c313bb",
            "cc47dd3b96d94519988ecd4d8f753bb4",
            "c11dcd593ce847ab8a04930fe47d0c5a",
            "2a6c53ce375b40618ba6cd7fe0cdc219",
            "0f8baaa8abbe4c129a2a16ea94dfe88e",
            "551e5afc227244cc84e2a7bdd674011b",
            "27e1ac28ce524be8b558e9dbb2096a25",
            "2b95fff5a40647f382a70e7e795215a0",
            "cf38fdface1e4e9b8228d5c7529719b5",
            "3f505422d2db4778891ed12d4eb66263",
            "d1aa9e5fb95a4f43b6ccac312899cedb",
            "fbaeb0be0c3f4322acff1941fb1f8f05",
            "dc9b8b942aff44188785f89bd04dcf02",
            "fea80a37f6254a64a608c52e4750b50e",
            "ef2c02bdd68547ca86db8ebbaa751a73",
            "025c7bd5d75b4904819d78469d58a8ed",
            "84e32786ca8844e09b3f285fb9f76cea",
            "f502c7e99b814205976c4fbea79f821e",
            "301ddfcacd9448b0b29867c2ec996b90",
            "6b1624c356bd4b60920a9f850d53e79d",
            "6a4f23864dec4bcca2c285813d2678b2",
            "67f8e576424d4c108231f04a0d3296c2",
            "c72b616b2fbe4a8e8e54c0c7951c78d0",
            "8adf72cabd8349ea9e5e3d23e4cadd73",
            "1a4dbb026e824809813ecfefa2a70dfe",
            "6fb1edb9ae194d628db60b964a20dfec",
            "1beafafdcf9c4a78aa5e1302f8b7438a",
            "239c2692a53d4bd0b83a966c9bb2c532",
            "0a84ad5372ff485dbde55a7dd7871023",
            "adada39a40a44e379bd17caf9af7a557",
            "4e78df3fd7a34c11920d561a637007bc",
            "c18956819c514e1ba65ff612b3c03611",
            "d9013517dcf448c596ab2f57bd58cf03",
            "0f2559d2d53d4c0380279dce7b453d3f",
            "222303cf14ef409c999b4b63b013dafe",
            "85073ae3e0e248a4bb850175a68e6d76",
            "954140d74de94b7e8044c1362e2b6782",
            "9490e4b110724803905128317f686655",
            "3a458108b59a4ae798309626a8adce94",
            "e182ad6dcedb4eff961dcd8ce4da3c0c",
            "3a8559f1cb974683a5a85673829e938b",
            "0866c4abf29044c8954c037893716c4e",
            "7f94e1c87fc043068c3d9f21a181dc8e",
            "3f79b87071cb41c0939e03d04fc149d4",
            "87f5356fff434f218fe6f9269ea6c6e6",
            "3abf02d9231b40838b047670d9bbd451",
            "f48a393179334ee8ad3aa31c84e42311",
            "56be98d5e72d4ab6a873f57d40b4366b",
            "43e8aa40c579483baf31c55ed56111e6",
            "f386137e402c4f049c44c10ff73619dc"
          ]
        },
        "outputId": "f697c34d-f39b-4284-a21d-5d0caf7553a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a33103c97b4d41e7b2425cc95a147ec1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e151a284d02642b7b32dd25a347044e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27e1ac28ce524be8b558e9dbb2096a25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f502c7e99b814205976c4fbea79f821e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a84ad5372ff485dbde55a7dd7871023"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e182ad6dcedb4eff961dcd8ce4da3c0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3120' max='3120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3120/3120 22:26, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.426300</td>\n",
              "      <td>0.394789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.336200</td>\n",
              "      <td>0.316574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.288100</td>\n",
              "      <td>0.279294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.237000</td>\n",
              "      <td>0.271155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.225600</td>\n",
              "      <td>0.259835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.198600</td>\n",
              "      <td>0.253134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.179100</td>\n",
              "      <td>0.246173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.162500</td>\n",
              "      <td>0.243497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.148800</td>\n",
              "      <td>0.242966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.141200</td>\n",
              "      <td>0.242257</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa Results:\n",
            "                   Attribute  Accuracy        F1    Recall  Precision\n",
            "0          dark_pigmentation  0.959135  0.832787  0.783951   0.888112\n",
            "1                       acne  0.981571  0.909804  0.892308   0.928000\n",
            "2                eye_contour  0.983974  0.928058  0.889655   0.969925\n",
            "3                homogeneity  0.947917  0.798762  0.821656   0.777108\n",
            "4              lack_firmness  0.927083  0.787879  0.757848   0.820388\n",
            "5              lack_radiance  0.915064  0.875000  0.868852   0.881235\n",
            "6                      pores  0.948718  0.883212  0.883212   0.883212\n",
            "7                 fine_lines  0.931891  0.891443  0.906494   0.876884\n",
            "8        wrinkles_fine-lines  0.936699  0.910734  0.897550   0.924312\n",
            "9               eye-wrinkles  0.980769  0.902439  0.895161   0.909836\n",
            "10             undereye-bags  0.977564  0.882353  0.937500   0.833333\n",
            "11                   generic  0.794872  0.835476  0.883152   0.792683\n",
            "12                     18-34  0.827724  0.852639  0.866295   0.839406\n",
            "13                     35-54  0.868590  0.849817  0.868914   0.831541\n",
            "14                     55-99  0.898237  0.797448  0.786164   0.809061\n",
            "15                       dry  0.902244  0.835135  0.800518   0.872881\n",
            "16                    normal  0.820513  0.748315  0.748315   0.748315\n",
            "17                      oily  0.926282  0.826415  0.790614   0.865613\n",
            "18               combination  0.792468  0.686820  0.674584   0.699507\n",
            "19          sensitivity-high  0.928686  0.760108  0.715736   0.810345\n",
            "20           sensitivity-low  0.894231  0.426087  0.371212   0.500000\n",
            "21            no_sensitivity  0.786859  0.849887  0.894299   0.809677\n",
            "22                      male  0.981571  0.916364  0.893617   0.940299\n",
            "23                    female  0.855769  0.897260  0.918224   0.877232\n",
            "24                   cleanse  0.961538  0.898305  0.894515   0.902128\n",
            "25                   prepare  0.927083  0.790805  0.738197   0.851485\n",
            "26                     treat  0.901442  0.890861  0.870017   0.912727\n",
            "27                  targeted  0.912660  0.868833  0.851415   0.886978\n",
            "28                      care  0.784455  0.780408  0.817094   0.746875\n",
            "29                moisturize  0.891827  0.917127  0.927950   0.906553\n",
            "30                   protect  0.887019  0.818999  0.803526   0.835079\n",
            "31                       day  0.837340  0.898753  0.939520   0.861377\n",
            "32                     night  0.838942  0.857951  0.870875   0.845404\n",
            "Average              Average  0.900325  0.836554  0.832089   0.843561\n",
            "\n",
            "Micro Average:\n",
            "Precision: 0.84, Recall: 0.85, F1: 0.85\n",
            "\n",
            "Macro Average:\n",
            "Precision: 0.84, Recall: 0.83, F1: 0.84\n",
            "\n",
            "Weighted Average:\n",
            "Precision: 0.84, Recall: 0.85, F1: 0.85\n",
            "\n",
            "Samples Average:\n",
            "Precision: 0.84, Recall: 0.86, F1: 0.84\n",
            "\n",
            "Total Carbon Emissions: 0.0060 kg CO2\n"
          ]
        }
      ]
    }
  ]
}